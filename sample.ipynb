{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c8a330",
   "metadata": {},
   "source": [
    "## **Library and Module Imports**  \n",
    "  \n",
    "The following code block imports all the essential libraries and frameworks required for the Clinical Intelligence System. These libraries support environment configuration, AI model integration, vector storage, data loading, evaluation, and performance metrics.  \n",
    "  \n",
    "### **Key Imports and Their Purpose**  \n",
    "  \n",
    "1. **Core AI and Environment Setup**  \n",
    "   - `import openai` – Provides access to OpenAI's API for natural language processing and model integration.  \n",
    "   - `from dotenv import load_dotenv` – Loads environment variables from a `.env` file to securely store API keys and configuration values.  \n",
    "  \n",
    "2. **Vector Storage and Embeddings**  \n",
    "   - `from langchain.vectorstores import Chroma` – Manages vector databases for semantic search and retrieval.  \n",
    "   - `from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI` – Integrates Azure-hosted OpenAI models for embeddings and chat-based language models.  \n",
    "  \n",
    "3. **Document Handling**  \n",
    "   - `from langchain.schema import Document` – Defines structured document objects for processing.  \n",
    "   - `from langchain_community.document_loaders.csv_loader import CSVLoader` – Loads CSV files into a document structure for further processing.  \n",
    "  \n",
    "4. **Utilities and Data Structures**  \n",
    "   - `from typing import List, Tuple, Dict` – Provides type hints for function parameters and return values.  \n",
    "   - `import numpy as np` – Supports numerical operations, arrays, and mathematical computations.  \n",
    "   - `import time` – Enables time tracking and performance measurement.  \n",
    "  \n",
    "5. **Retry Mechanisms**  \n",
    "   - `from tenacity import retry, stop_after_attempt, wait_random_exponential` – Implements robust retry logic to handle API timeouts and transient failures.  \n",
    "  \n",
    "6. **Evaluation Framework**  \n",
    "   - `from deepeval.models.base_model import DeepEvalBaseLLM` – Base class for evaluating large language models.  \n",
    "   - `from deepeval.test_case import LLMTestCase, LLMTestCaseParams` – Defines structured test cases for model evaluation.  \n",
    "   - `from deepeval import evaluate as deepeval_evaluate` – Runs evaluation processes for AI model outputs.  \n",
    "  \n",
    "7. **Evaluation Metrics**  \n",
    "   - `from deepeval.metrics import (ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric, AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric)`    \n",
    "     - **ContextualPrecisionMetric** – Measures the accuracy of retrieved information within the given context.    \n",
    "     - **ContextualRecallMetric** – Measures how much relevant context is retrieved.    \n",
    "     - **ContextualRelevancyMetric** – Evaluates the contextual fit of retrieved information.    \n",
    "     - **AnswerRelevancyMetric** – Assesses how relevant the generated answer is to the question.    \n",
    "     - **FaithfulnessMetric** – Ensures the answer is factually grounded in the source material.    \n",
    "     - **HallucinationMetric** – Detects fabricated or unsupported information in responses.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Summary:**    \n",
    "This set of imports lays the groundwork for:  \n",
    "- **NLP processing** (OpenAI, Azure OpenAI, LangChain)  \n",
    "- **Data management** (Chroma DB, CSV loaders, Document schema)  \n",
    "- **Robustness** (retry mechanisms)  \n",
    "- **Evaluation** (DeepEval metrics and test cases)    \n",
    "These tools collectively enable the system to process clinical queries, retrieve relevant data, generate responses, and evaluate their quality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np  \n",
    "import time\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval import evaluate as deepeval_evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a5099",
   "metadata": {},
   "source": [
    "### Create Model Client and Set Up Authentication\n",
    "\n",
    "The following code initializes the UAIS environment to establish a secure connection with the Azure OpenAI service. It handles authentication by retrieving the necessary access token and configures the embedding function to generate vector representations for input text. This setup enables downstream tasks such as semantic search, similarity comparison, and other embedding-based applications.\n",
    "\n",
    "| Requirement           | Description                                                        |  \n",
    "|-----------------------|--------------------------------------------------------------------|  \n",
    "| Large Language Models (LLM) | OpenAI LLM API (`gpt-4o-mini_2024-07-18`)                           |  \n",
    "| Embedding Models      | Preferred embedding model is `text-embedding-3-small_1`            |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication:\n",
    "import httpx\n",
    "\n",
    "auth = \"https://api.com/oauth2/token\"\n",
    "client_id = dbutils.secrets.get(scope = \"AIML\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"AIML\", key = \"client_secret\")\n",
    "scope = \"https://api.com/.default\"\n",
    "grant_type = \"client_credentials\"\n",
    "async with httpx.AsyncClient() as client:\n",
    "    body = {\n",
    "        \"grant_type\": grant_type,\n",
    "        \"scope\": scope,\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    resp = await client.post(auth, headers=headers, data=body, timeout=120)\n",
    "    token = resp.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "load_dotenv(\"./Data/vars.env\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"MODEL_ENDPOINT\"]\n",
    "OPENAI_API_VERSION = os.environ[\"API_VERSION\"]\n",
    "CHAT_DEPLOYMENT_NAME = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = os.environ[\"EMBEDDINGS_MODEL_NAME\"]\n",
    "\n",
    "chat_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "embeddings_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={ \n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4e58b",
   "metadata": {},
   "source": [
    "## Azure OpenAI Model & Embeddings Setup  \n",
    "  \n",
    "This section initializes the Azure OpenAI resources required for the RAG pipeline:  \n",
    "  \n",
    "- **`AzureChatOpenAI`** – Configures a chat-based LLM endpoint using Azure OpenAI, enabling conversational interactions and contextual responses.  \n",
    "- **`AzureOpenAIEmbeddings`** – Sets up an embedding model to convert text into high-dimensional vectors for semantic search and retrieval.  \n",
    "- Both components share:  \n",
    "  - The same **API version** and **Azure endpoint**.  \n",
    "  - **Azure AD token authentication** for secure access.  \n",
    "  - Custom **`projectId`** in request headers for project-level tracking.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\"projectId\": PROJECT_ID},\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8746e5c",
   "metadata": {},
   "source": [
    "### Tiktoken Cache Configuration\n",
    " \n",
    "> This code sets up a custom cache directory for Tiktoken by defining `TIKTOKEN_CACHE_DIR` as an environment variable.  \n",
    "> Local caching of tokenization results enhances performance by avoiding repeated computation during recurring embedding or tokenization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062218c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken_cache_dir = os.path.abspath(\"./.setup/tiktoken_cache/\")\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = tiktoken_cache_dir\n",
    "\n",
    "# we have to disable telemetry to use ChromaDB\n",
    "# See here for more information: https://docs.trychroma.com/docs/overview/telemetry\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"]=\"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3946aac",
   "metadata": {},
   "source": [
    "## DocumentProcessor Class  \n",
    "  \n",
    "The `DocumentProcessor` class leverages **LangChain's CSVLoader** to efficiently ingest CSV datasets and convert them into LangChain `Document` objects. This design ensures:  \n",
    "  \n",
    "- **Built-in CSV parsing** with automatic conversion to Document objects.    \n",
    "- **Native LangChain document structure** for smooth integration into RAG pipelines.    \n",
    "- **Standardized metadata extraction** adhering to LangChain conventions.    \n",
    "- **Direct compatibility** with LangChain’s ecosystem of loaders, retrievers, and vector stores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Streamlined Document Processing Engine for RAG pipeline using LangChain CSVLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentProcessor.\n",
    "        \"\"\"\n",
    "        self.langchain_docs = []  # List of LangChain Document objects for RAG\n",
    "        \n",
    "    def load_csv_with_langchain(self, csv_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load CSV data using LangChain CSVLoader.\n",
    "        \n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file to load\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        # Configure CSVLoader with our dataset structure\n",
    "        loader = CSVLoader(\n",
    "            file_path=csv_path,\n",
    "            source_column=\"document_url\",  # Use document_url as source\n",
    "            metadata_columns=[\"document_id\", \"document_url\"],  # Include these in metadata\n",
    "            content_columns=[\"context\"]  # Use context as main content\n",
    "        )\n",
    "        \n",
    "        # Load documents using LangChain\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"Successfully loaded {len(documents)} document chunks from CSV using LangChain CSVLoader.\")\n",
    "        return documents\n",
    "        \n",
    "    def load_dataset(self, dataset_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load documents from CSV using LangChain CSVLoader and return Documents ready for vector storage.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Path to the CSV dataset file\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the dataset file is not found\n",
    "            Exception: If there's an error loading the CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(dataset_path):\n",
    "                raise FileNotFoundError(f\"Dataset file not found: {dataset_path}\")\n",
    "            \n",
    "            print(f\"📁 Loading dataset from: {dataset_path}\")\n",
    "            \n",
    "            # Use LangChain CSVLoader \n",
    "            self.langchain_docs = self.load_csv_with_langchain(dataset_path)\n",
    "            \n",
    "            print(f\"✅ Dataset loaded successfully using LangChain CSVLoader\")\n",
    "            print(f\"📚 Loaded {len(self.langchain_docs)} LangChain Document objects\")\n",
    "            \n",
    "            # Display first document for verification\n",
    "            if self.langchain_docs:\n",
    "                print(f\"\\n🔍 Sample LangChain Document:\")\n",
    "                sample_doc = self.langchain_docs[0]\n",
    "                print(f\"   Content: {sample_doc.page_content[:100]}...\")\n",
    "                print(f\"   Metadata: {sample_doc.metadata}\")\n",
    "            \n",
    "            return self.langchain_docs\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"❌ File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading dataset with CSVLoader: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_langchain_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Get the loaded LangChain Documents ready for vector storage.\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        return self.langchain_docs\n",
    "    \n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get the number of loaded documents.\"\"\"\n",
    "        return len(self.langchain_docs)\n",
    "    \n",
    "    def display_dataset_info(self) -> None:\n",
    "        \"\"\"Display essential information about the loaded dataset for RAG.\"\"\"\n",
    "        if not self.langchain_docs:\n",
    "            print(\"⚠️ No documents loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📊 === Dataset Information for RAG Pipeline ===\")\n",
    "        print(f\"📚 Total documents: {len(self.langchain_docs)}\")\n",
    "        \n",
    "        if self.langchain_docs:\n",
    "            # Show sample LangChain document structure\n",
    "            print(f\"\\n📄 === Sample LangChain Document ===\")\n",
    "            sample_doc = self.langchain_docs[0]\n",
    "            print(f\"page_content: {sample_doc.page_content[:200]}...\")\n",
    "            print(f\"metadata: {sample_doc.metadata}\")\n",
    "            \n",
    "            # Show content length statistics\n",
    "            content_lengths = [len(doc.page_content) for doc in self.langchain_docs]\n",
    "            print(f\"\\n📊 === Content Statistics ===\")\n",
    "            print(f\"Average content length: {sum(content_lengths) / len(content_lengths):.0f} characters\")\n",
    "            print(f\"Shortest document: {min(content_lengths)} characters\")\n",
    "            print(f\"Longest document: {max(content_lengths)} characters\")\n",
    "\n",
    "print(\"✅ LangChain CSVLoader-based DocumentProcessor defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775d180",
   "metadata": {},
   "source": [
    "## Vector Store Creation with Check-and-Reuse Logic  \n",
    "  \n",
    "This section defines a **smart vector store creation pipeline** for efficient embedding management.  \n",
    "  \n",
    "- **Purpose:**    \n",
    "  - Checks if a persisted ChromaDB vector store already exists.    \n",
    "  - **If found:** Loads and reuses existing embeddings without regeneration.    \n",
    "  - **If not found:** Creates a new vector store from the loaded documents, generates embeddings, and persists the store.  \n",
    "  \n",
    "- **Key Features:**    \n",
    "  - Uses `DocumentProcessor` to retrieve preloaded LangChain documents.    \n",
    "  - Embeddings are created via the configured Azure OpenAI embedding model.    \n",
    "  - Ensures persistence in a specified directory for later reuse.    \n",
    "  - Includes diagnostic statistics (vector count, embedding dimensions, range, and norms).    \n",
    "  - Built with retry logic to handle transient failures during creation or loading.  \n",
    "  \n",
    "- **Benefit:**    \n",
    "  This approach saves computation time and cost by **avoiding redundant embedding generation** while still supporting full regeneration when required.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3aa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Creation Pipeline with Check-and-Reuse Logic\n",
    "@retry(wait=wait_random_exponential(min=45, max=120), stop=stop_after_attempt(6))\n",
    "def create_vector_store(processor: DocumentProcessor, collection_name: str = \"clinical_intelligence\", persist_directory: str = \"./Data/clinical_rag.db\"):\n",
    "    \"\"\"\n",
    "    Create embeddings and store in ChromaDB vector store with intelligent reuse.\n",
    "    \n",
    "    Purpose:\n",
    "    - Checks whether a vector store already exists in the specified directory\n",
    "    - If it does, loads and reuses the existing vector store\n",
    "    - If it doesn't, creates a new vector store from the provided documents and persists it\n",
    "    \n",
    "    Args:\n",
    "        processor (DocumentProcessor): Loaded document processor\n",
    "        collection_name (str): Name for the ChromaDB collection\n",
    "        persist_directory (str): Directory path for ChromaDB persistence\n",
    "        \n",
    "    Returns:\n",
    "        Chroma: ChromaDB vector store with embedded documents\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 === Clinical Intelligence Pipeline - Step 2: Vector Store Creation ===\")\n",
    "    print(f\"📂 Checking persist directory: {persist_directory}\")\n",
    "    print(f\"🗂️ Collection name: {collection_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if vector store already exists\n",
    "        if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "            print(f\"📋 Existing vector store found in: {persist_directory}\")\n",
    "            print(f\"🔄 Loading existing vector store...\")\n",
    "            \n",
    "            # Load existing vector store\n",
    "            vector_store = Chroma(\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Existing vector store loaded successfully!\")\n",
    "            print(f\"📚 Found {vector_store._collection.count()} existing vectors\")\n",
    "            print(f\"🔄 Reusing existing embeddings (no regeneration needed)\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"📭 No existing vector store found\")\n",
    "            print(f\"🆕 Creating new vector store from documents...\")\n",
    "            print(f\"📊 Processing {processor.get_document_count()} documents for embedding...\")\n",
    "            \n",
    "            # Get documents from processor\n",
    "            documents = processor.get_langchain_documents()\n",
    "            \n",
    "            if not documents:\n",
    "                raise ValueError(\"No documents loaded. Run clinical_intelligence_pipeline first.\")\n",
    "            \n",
    "            # Create new vector store with embeddings\n",
    "            print(f\"🔍 Creating embeddings using text-embedding-3-small model...\")\n",
    "            print(f\"💾 Storing vectors in ChromaDB collection: '{collection_name}'\")\n",
    "            \n",
    "            # Create vector store - this will automatically generate embeddings and store them\n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=embeddings,\n",
    "                collection_name=collection_name,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ New vector store created successfully!\")\n",
    "            print(f\"📚 Embedded and stored {len(documents)} documents\")\n",
    "            print(f\"💾 Persisted to: {persist_directory}\")\n",
    "        \n",
    "        # Display common statistics\n",
    "        print(f\"\\n📊 === Vector Store Statistics ===\")\n",
    "        print(f\"Total vectors: {vector_store._collection.count()}\")\n",
    "        print(f\"Collection name: {collection_name}\")\n",
    "        print(f\"Persist directory: {persist_directory}\")\n",
    "        print(f\"Embedding model: text-embedding-3-small\")\n",
    "        \n",
    "        # Test embedding dimension using actual document content from CSV\n",
    "        try:\n",
    "            # Use first document's content for testing embedding\n",
    "            documents = processor.get_langchain_documents()\n",
    "            if documents and len(documents) > 0:\n",
    "                # Take first 100 characters from first document for testing\n",
    "                test_content = documents[0].page_content[:100]\n",
    "                test_embedding = embeddings.embed_query(test_content)\n",
    "                print(f\"Vector dimension: {len(test_embedding)}\")\n",
    "                print(f\"Test content: '{test_content[:50]}...'\")\n",
    "                print(f\"Sample embedding values (first 10): {test_embedding[:10]}\")\n",
    "                print(f\"Embedding range: [{min(test_embedding):.6f}, {max(test_embedding):.6f}]\")\n",
    "                print(f\"Embedding norm: {sum(x*x for x in test_embedding)**0.5:.6f}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No documents available for embedding test\")\n",
    "                print(f\"Vector dimension: Unknown (no test performed)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Vector dimension: Available in collection (test failed: {e})\")\n",
    "        \n",
    "        return vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with vector store: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"✅ Smart vector store creation function defined successfully!\")\n",
    "print(\"🔄 Supports both new creation and existing vector store reuse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccca888",
   "metadata": {},
   "source": [
    "## Complete Clinical Intelligence Pipeline  \n",
    "  \n",
    "This function orchestrates the **full data-to-vector pipeline** for the Clinical Intelligence system, combining both document ingestion and vectorization.  \n",
    "  \n",
    "- **Purpose:**    \n",
    "  - Streamline the process of loading CSV-based clinical data into LangChain `Document` objects.    \n",
    "  - Automatically embed documents into a ChromaDB vector store for semantic search and retrieval.  \n",
    "  \n",
    "- **Workflow Steps:**    \n",
    "  1. **Document Processing:**    \n",
    "     - Instantiates a `DocumentProcessor`.    \n",
    "     - Loads the dataset via LangChain's `CSVLoader`.    \n",
    "     - Prepares documents for downstream vectorization.  \n",
    "  2. **Vector Store Creation:**    \n",
    "     - Invokes the smart `create_vector_store()` function.    \n",
    "     - Either reuses an existing ChromaDB store or generates embeddings for new documents.    \n",
    "     - Persists the vector store for future queries.  \n",
    "  \n",
    "- **Output:**    \n",
    "  Returns a tuple of `(DocumentProcessor, Chroma)` for immediate use in query workflows.  \n",
    "  \n",
    "- **Benefit:**    \n",
    "  Offers a **one-command execution** for setting up the RAG-ready clinical intelligence environment, ensuring consistency between document loading and embedding stages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Clinical Intelligence Pipeline - Data Loading + Vectorization\n",
    "def complete_clinical_intelligence_pipeline(dataset_path: str, persist_directory: str, collection_name: str = \"clinical_intelligence\"):\n",
    "    \"\"\"\n",
    "    Complete Clinical Intelligence Pipeline that handles both document processing and vector store creation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the CSV dataset file\n",
    "        persist_directory (str): Directory path for ChromaDB persistence\n",
    "        collection_name (str): Name for the ChromaDB collection\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DocumentProcessor, Chroma) - processor and vector store\n",
    "    \"\"\"\n",
    "    print(f\"🏥 === Complete Clinical Intelligence Pipeline ===\")\n",
    "    print(f\"📂 Dataset path: {dataset_path}\")\n",
    "    print(f\"💾 Persist directory: {persist_directory}\")\n",
    "    print(f\"🗂️ Collection name: {collection_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Document Processing\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🚀 === Step 1: Document Processing ===\")\n",
    "        print(f\"📂 Dataset path: {dataset_path}\")\n",
    "        \n",
    "        # Initialize the document processor\n",
    "        processor = DocumentProcessor()\n",
    "        \n",
    "        # Load the dataset using LangChain CSVLoader\n",
    "        langchain_documents = processor.load_dataset(dataset_path)\n",
    "        print(f\"\\n🎉 Successfully loaded {processor.get_document_count()} LangChain Documents using CSVLoader!\")\n",
    "        print(f\"✅ Documents are ready for vector storage and embeddings\")\n",
    "        print(f\"🔄 Using same loading pattern as PDF documents for consistency\")\n",
    "        \n",
    "        # Step 2: Vector Store Creation\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🚀 === Step 2: Vector Store Creation ===\")\n",
    "        vector_store = create_vector_store(\n",
    "            processor, \n",
    "            collection_name=collection_name, \n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 === Complete Clinical Intelligence Pipeline Finished ===\")\n",
    "        print(f\"✅ Step 1: Document processing completed - {processor.get_document_count()} documents loaded\")\n",
    "        print(f\"✅ Step 2: Vector store creation completed - {vector_store._collection.count()} vectors stored\")\n",
    "        print(f\"🚀 Clinical Intelligence system ready for queries!\")\n",
    "        \n",
    "        return processor, vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in complete pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration variables\n",
    "dataset_path = \"./Data/capstone1_rag_dataset.csv\"\n",
    "persist_directory = \"./Data/clinical_rag.db\"\n",
    "collection_name = \"clinical_intelligence_v1\"\n",
    "\n",
    "# Execute Complete Pipeline in One Call\n",
    "processor, vector_store = complete_clinical_intelligence_pipeline(\n",
    "    dataset_path=dataset_path,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafbf0c8",
   "metadata": {},
   "source": [
    "## Retrieval Strategy Exploration  \n",
    "  \n",
    "In this section, we evaluate multiple retrieval strategies to identify the most effective approach for clinical document retrieval. Each method balances precision, recall, and relevance differently, aiming to ensure that retrieved documents are both accurate and contextually aligned with the query.  \n",
    "  \n",
    "### 🎯 Retrieval Strategies to Compare  \n",
    "  \n",
    "- **Semantic Search**    \n",
    "  Utilizes pure vector similarity to retrieve documents that are semantically closest to the query, enabling context-aware matching beyond exact keywords.  \n",
    "  \n",
    "- **Semantic Search with Threshold Filtering**    \n",
    "  Builds on semantic similarity but applies a relevance score cutoff, ensuring only highly relevant documents are returned and reducing noise.  \n",
    "  \n",
    "- **Hybrid Search**    \n",
    "  Combines keyword-based retrieval with semantic similarity to capture both exact matches and contextual relevance, improving coverage for diverse query types.  \n",
    "  \n",
    "- **Re-ranking based on Relevance Scores**    \n",
    "  Retrieves a broader set of candidates, then applies a secondary scoring mechanism to reorder results for optimal relevance and user satisfaction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4578a2",
   "metadata": {},
   "source": [
    "### Strategy 1: Pure Semantic Search  \n",
    "  \n",
    "**Purpose:**    \n",
    "This approach retrieves documents purely based on vector similarity, leveraging semantic embeddings to capture contextual meaning rather than relying on exact keyword matches. It is designed to surface conceptually relevant results even when terminology differs between the query and the source text.  \n",
    "  \n",
    "**Evaluation Summary:**    \n",
    "- **Query:** \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"    \n",
    "- **Results Found:** 5 documents    \n",
    "- **Observation:** Returned top-ranked documents with strong contextual alignment to insulin-related conditions and high blood sugar. However, some results included rare syndromes, which may be less directly relevant to the query focus.  \n",
    "  \n",
    "**Key Takeaway:**    \n",
    "Pure semantic search effectively identifies related medical conditions and context-rich information but may require additional filtering or re-ranking to prioritize the most clinically relevant documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchStrategy:\n",
    "    \"\"\"Pure semantic search using vector similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, name=\"Semantic Search\"):\n",
    "        self.vector_store = vector_store\n",
    "        self.name = name\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[any, float]]:\n",
    "        \"\"\"\n",
    "        Perform pure semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: Documents with similarity scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform similarity search with scores\n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        return results, search_time\n",
    "    \n",
    "    def evaluate_query(self, query: str, k: int = 5) -> Dict:\n",
    "        \"\"\"Evaluate a single query and return detailed results.\"\"\"\n",
    "        results, search_time = self.search(query, k)\n",
    "        \n",
    "        evaluation = {\n",
    "            'strategy': self.name,\n",
    "            'query': query,\n",
    "            'num_results': len(results),\n",
    "            'search_time': search_time,\n",
    "            'results': []\n",
    "        }\n",
    "        \n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            evaluation['results'].append({\n",
    "                'rank': i + 1,\n",
    "                'similarity_score': score,\n",
    "                'document_id': doc.metadata.get('document_id', 'N/A'),\n",
    "                'content_preview': doc.page_content[:150] + \"...\",\n",
    "                'source': doc.metadata.get('document_url', 'N/A')\n",
    "            })\n",
    "        \n",
    "        return evaluation\n",
    "\n",
    "# Initialize semantic search strategy\n",
    "semantic_search = SemanticSearchStrategy(vector_store)\n",
    "\n",
    "# Test with a sample query\n",
    "sample_query = \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"\n",
    "evaluation = semantic_search.evaluate_query(sample_query)\n",
    "\n",
    "print(f\"🔍 === Strategy 1: {evaluation['strategy']} ===\")\n",
    "print(f\"Query: '{evaluation['query']}'\")\n",
    "print(f\"Results found: {evaluation['num_results']}\")\n",
    "print(f\"Search time: {evaluation['search_time']:.4f} seconds\")\n",
    "\n",
    "print(f\"\\n📄 Top Results:\")\n",
    "for result in evaluation['results'][:3]:\n",
    "    print(f\"\\n  Rank {result['rank']} (Score: {result['similarity_score']:.4f})\")\n",
    "    print(f\"  Content: {result['content_preview']}\")\n",
    "    print(f\"  Document ID: {result['document_id']}\")\n",
    "\n",
    "print(f\"\\n✅ Pure Semantic Search strategy implemented and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d858a8b",
   "metadata": {},
   "source": [
    "### Strategy 2: Adaptive Threshold Semantic Search  \n",
    "  \n",
    "**Purpose:**    \n",
    "This method enhances pure semantic search by applying a dynamic relevance score threshold. The threshold is automatically calculated from the score distribution for each query, based on a chosen percentile. This ensures that only the top proportion of most relevant results are retained, filtering out lower-quality matches.  \n",
    "  \n",
    "**Evaluation Summary:**    \n",
    "- **Query:** \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"    \n",
    "- **Approach:** Tested at 70%, 80%, and 90% percentiles (higher percentile = more selective).    \n",
    "- **Key Results:**    \n",
    "  - **70% percentile:** Kept 3 out of 10 results (Threshold: 0.4051, Time: 2.11s)    \n",
    "  - **80% percentile:** Kept 2 out of 10 results (Threshold: 0.4972, Time: 0.53s)    \n",
    "  - **90% percentile:** Kept 1 out of 10 results (Threshold: 0.7934, Time: 0.45s)    \n",
    "  \n",
    "**Observations:**    \n",
    "- Adaptive filtering drastically reduces noise and focuses on the most relevant matches.    \n",
    "- Higher percentiles yield fewer but more precise results, at the cost of coverage.    \n",
    "- Search times were significantly faster than pure semantic search, particularly at higher selectivity levels.  \n",
    "  \n",
    "**Key Takeaway:**    \n",
    "This approach is effective for high-precision retrieval in clinical contexts where irrelevant results could be misleading. The percentile parameter provides flexible control over selectivity, allowing tuning for different use cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f026617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveThresholdSemanticSearch(SemanticSearchStrategy):\n",
    "    \"\"\"\n",
    "    Semantic search with adaptive relevance score threshold filtering.\n",
    "    Threshold is chosen dynamically from score distribution based on a percentile.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, percentile: float = 80, name=\"Adaptive Threshold Semantic Search\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vector_store: LangChain-compatible vector store (e.g., Chroma)\n",
    "            percentile: Percentile cutoff for keeping results (0-100)\n",
    "                        Example: 80 = keep top 20% most similar results\n",
    "        \"\"\"\n",
    "        super().__init__(vector_store, name)\n",
    "        self.percentile = percentile\n",
    "        self.dynamic_threshold = None  # Will be set on first query\n",
    "    \n",
    "    def search(self, query: str, k: int = 10) -> Tuple[List[Tuple[any, float]], float]:\n",
    "        \"\"\"\n",
    "        Perform semantic search, compute dynamic threshold, and filter results.\n",
    "        Returns: (filtered_results, search_time)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # First get regular similarity search results (without threshold filtering)\n",
    "        try:\n",
    "            import warnings\n",
    "            import sys\n",
    "            import os\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                old_stdout = sys.stdout\n",
    "                sys.stdout = open(os.devnull, 'w')\n",
    "                try:\n",
    "                    # Use similarity_search_with_score to get distance scores first\n",
    "                    distance_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "                    # Convert distance to relevance scores using a better approach\n",
    "                    results = []\n",
    "                    \n",
    "                    # Extract all distances to normalize them properly\n",
    "                    distances = [distance for _, distance in distance_results]\n",
    "                    \n",
    "                    # Use rank-based scoring or inverse distance scoring\n",
    "                    if distances:\n",
    "                        max_distance = max(distances)\n",
    "                        min_distance = min(distances)\n",
    "                        distance_range = max_distance - min_distance\n",
    "                        \n",
    "                        for doc, distance in distance_results:\n",
    "                            if distance_range > 0:\n",
    "                                # Normalize distance to 0-1 range, then invert (lower distance = higher relevance)\n",
    "                                normalized_distance = (distance - min_distance) / distance_range\n",
    "                                relevance_score = 1.0 - normalized_distance\n",
    "                            else:\n",
    "                                # All distances are the same\n",
    "                                relevance_score = 1.0\n",
    "                            results.append((doc, relevance_score))\n",
    "                    \n",
    "                finally:\n",
    "                    sys.stdout.close()\n",
    "                    sys.stdout = old_stdout\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Search error: {e}\")\n",
    "            results = []\n",
    "        \n",
    "        if not results:\n",
    "            self.dynamic_threshold = None\n",
    "            return [], time.time() - start_time\n",
    "        \n",
    "        # Extract scores for threshold calculation\n",
    "        scores = [score for _, score in results]\n",
    "        \n",
    "        # Compute dynamic threshold based on requested percentile\n",
    "        # For percentile=80, we keep top 20% (scores above 80th percentile)\n",
    "        self.dynamic_threshold = float(np.percentile(scores, self.percentile))\n",
    "        \n",
    "        # Keep only results above threshold\n",
    "        filtered_results = [(doc, score) for doc, score in results if score >= self.dynamic_threshold]\n",
    "        \n",
    "        return filtered_results, time.time() - start_time\n",
    "    \n",
    "    def evaluate_query(self, query: str, k: int = 10) -> Dict:\n",
    "        \"\"\"Evaluate query with adaptive threshold filtering details.\"\"\"\n",
    "        results, search_time = self.search(query, k)\n",
    "        \n",
    "        threshold_str = f\"{self.dynamic_threshold:.4f}\" if self.dynamic_threshold is not None else \"N/A\"\n",
    "        \n",
    "        evaluation = {\n",
    "            'strategy': f\"{self.name} (percentile={self.percentile}%, threshold={threshold_str})\",\n",
    "            'query': query,\n",
    "            'initial_k': k,\n",
    "            'num_results_after_filtering': len(results),\n",
    "            'search_time': search_time,\n",
    "            'dynamic_threshold': self.dynamic_threshold,\n",
    "            'results': []\n",
    "        }\n",
    "        \n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            evaluation['results'].append({\n",
    "                'rank': i + 1,\n",
    "                'relevance_score': score,\n",
    "                'document_id': doc.metadata.get('document_id', 'N/A'),\n",
    "                'content_preview': doc.page_content[:150] + \"...\",\n",
    "                'source': doc.metadata.get('document_url', 'N/A')\n",
    "            })\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "\n",
    "# Test adaptive threshold search with different percentiles\n",
    "percentiles_to_test = [70, 80, 90]\n",
    "\n",
    "print(f\"\\n🔍 === Strategy 2b: Adaptive Threshold Semantic Search ===\")\n",
    "print(f\"📋 Query: '{sample_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for percentile in percentiles_to_test:\n",
    "    adaptive_search = AdaptiveThresholdSemanticSearch(vector_store, percentile=percentile)\n",
    "    evaluation = adaptive_search.evaluate_query(sample_query)\n",
    "    \n",
    "    threshold_display = evaluation['dynamic_threshold']\n",
    "    if threshold_display is None:\n",
    "        threshold_display = \"N/A\"\n",
    "    else:\n",
    "        threshold_display = f\"{threshold_display:.4f}\"\n",
    "    \n",
    "    print(f\"\\n🎯 Percentile: {percentile}% (keeps top {100-percentile}% of results)\")\n",
    "    print(f\"   Dynamic threshold: {threshold_display}\")\n",
    "    print(f\"   Results kept: {evaluation['num_results_after_filtering']}/{evaluation['initial_k']}\")\n",
    "    print(f\"   Search time: {evaluation['search_time']:.4f}s\")\n",
    "    \n",
    "    if evaluation['results']:\n",
    "        print(\"   📑 Top results:\")\n",
    "        for r in evaluation['results'][:2]:  # Show top 2\n",
    "            print(f\"      • Score: {r['relevance_score']:.4f} | Doc ID: {r['document_id']}\")\n",
    "            print(f\"        Preview: {r['content_preview'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   ❌ No results passed the adaptive filter\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"✅ Adaptive threshold search completed!\")\n",
    "print(f\"💡 Higher percentiles = more selective filtering\")\n",
    "print(f\"🎯 Adaptive thresholds adjust based on actual score distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab647bd",
   "metadata": {},
   "source": [
    "### Strategy 3: Hybrid Search (Semantic + Keyword/BM25)  \n",
    "  \n",
    "**Purpose:**    \n",
    "Hybrid Search combines the strengths of **semantic embeddings** and **keyword-based BM25** retrieval to improve both coverage and precision. Semantic search captures contextual meaning, while BM25 ensures that exact keyword matches are prioritized. A weighted scoring mechanism merges both ranking signals, enabling more balanced retrieval performance.  \n",
    "  \n",
    "**Evaluation Summary:**    \n",
    "- **Query:** \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"    \n",
    "- **Weights:** Semantic (70%), Keyword/BM25 (30%)    \n",
    "- **Top Results Comparison:**    \n",
    "  - **Semantic Search:** Focused on semantically related rare conditions (DocIDs 297, 328, 414)    \n",
    "  - **BM25 Keyword Search:** Returned the same top two results but with stronger keyword match scores, plus an additional different third result (DocID 711)    \n",
    "- **Final Hybrid Ranking:**    \n",
    "  1. **DocID 297** – Score: 1.00    \n",
    "  2. **DocID 328** – Score: 0.737    \n",
    "  3. **DocID 414** – Score: 0.231    \n",
    "  \n",
    "**Observations:**    \n",
    "- Hybrid ranking aligned top results from both methods, ensuring that documents relevant both contextually and lexically are prioritized.    \n",
    "- Retained the same top 2 documents across all methods, but improved ranking confidence by combining scores.    \n",
    "- Helps mitigate cases where semantic search may include tangential results or BM25 misses conceptually relevant ones.  \n",
    "  \n",
    "**Key Takeaway:**    \n",
    "The hybrid approach offers a **balanced retrieval strategy** that benefits from semantic understanding while maintaining the precision of keyword matching, making it a strong candidate for clinical search tasks where both terminology and context matter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re  \n",
    "import time  \n",
    "import numpy as np  \n",
    "from rank_bm25 import BM25Okapi  \n",
    "from typing import List, Tuple, Dict  \n",
    "  \n",
    "# --- Built-in English stopwords ---  \n",
    "ENGLISH_STOPWORDS = {  \n",
    "    \"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",  \n",
    "    \"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",  \n",
    "    \"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",  \n",
    "    \"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",  \n",
    "    \"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",  \n",
    "    \"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",  \n",
    "    \"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",  \n",
    "    \"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",  \n",
    "    \"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",  \n",
    "    \"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",  \n",
    "    \"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",  \n",
    "    \"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"  \n",
    "}  \n",
    "  \n",
    "# --- Simple tokenizer ---  \n",
    "def simple_tokenize(text: str) -> List[str]:  \n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text.lower())  \n",
    "    return [t for t in tokens if t not in ENGLISH_STOPWORDS]  \n",
    "  \n",
    "# --- Hybrid Search Class ---  \n",
    "class HybridSearchStrategy:  \n",
    "    def __init__(self, vector_store, processor, semantic_weight=0.7, keyword_weight=0.3):  \n",
    "        self.vector_store = vector_store  \n",
    "        self.processor = processor  \n",
    "        self.semantic_weight = semantic_weight  \n",
    "        self.keyword_weight = keyword_weight  \n",
    "  \n",
    "        # Prepare documents  \n",
    "        self.documents = processor.get_langchain_documents()  \n",
    "        self.doc_texts = [doc.page_content for doc in self.documents]  \n",
    "        self.doc_id_to_index = {doc.metadata.get('document_id'): i for i, doc in enumerate(self.documents)}  \n",
    "  \n",
    "        # Tokenize and build BM25 index  \n",
    "        tokenized_docs = [simple_tokenize(text) for text in self.doc_texts]  \n",
    "        self.bm25 = BM25Okapi(tokenized_docs)  \n",
    "        print(f\"📊 BM25 index created for {len(tokenized_docs)} documents (offline mode)\")  \n",
    "  \n",
    "    def keyword_search(self, query: str, k: int = 10):  \n",
    "        \"\"\"BM25 keyword search\"\"\"  \n",
    "        query_tokens = simple_tokenize(query)  \n",
    "        bm25_scores = self.bm25.get_scores(query_tokens)  \n",
    "        top_indices = np.argsort(bm25_scores)[::-1][:k]  \n",
    "        return [(idx, bm25_scores[idx]) for idx in top_indices if bm25_scores[idx] > 0]  \n",
    "  \n",
    "    def _normalize_scores(self, score_dict: Dict[int, float]) -> Dict[int, float]:  \n",
    "        if not score_dict:  \n",
    "            return {}  \n",
    "        min_score = min(score_dict.values())  \n",
    "        max_score = max(score_dict.values())  \n",
    "        if max_score == min_score:  \n",
    "            return {k: 0.0 for k in score_dict}  \n",
    "        return {k: (v - min_score) / (max_score - min_score) for k, v in score_dict.items()}  \n",
    "  \n",
    "    def search(self, query: str, k: int = 5):  \n",
    "        \"\"\"Hybrid search combining semantic + keyword scores\"\"\"  \n",
    "        start_time = time.time()  \n",
    "  \n",
    "        # Semantic search  \n",
    "        semantic_results = self.vector_store.similarity_search_with_relevance_scores(query, k=k*2)  \n",
    "        semantic_scores = {}  \n",
    "        for doc, score in semantic_results:  \n",
    "            idx = self.doc_id_to_index.get(doc.metadata.get('document_id'))  \n",
    "            if idx is not None:  \n",
    "                semantic_scores[idx] = float(score)  \n",
    "  \n",
    "        # Keyword search  \n",
    "        keyword_results = self.keyword_search(query, k=k*2)  \n",
    "        keyword_scores = {idx: score for idx, score in keyword_results}  \n",
    "  \n",
    "        # Normalize both scores to 0–1  \n",
    "        semantic_scores = self._normalize_scores(semantic_scores)  \n",
    "        keyword_scores = self._normalize_scores(keyword_scores)  \n",
    "  \n",
    "        # Combine weighted scores  \n",
    "        combined_scores = {}  \n",
    "        for idx in set(semantic_scores.keys()) | set(keyword_scores.keys()):  \n",
    "            combined_scores[idx] = (  \n",
    "                self.semantic_weight * semantic_scores.get(idx, 0.0) +  \n",
    "                self.keyword_weight * keyword_scores.get(idx, 0.0)  \n",
    "            )  \n",
    "  \n",
    "        # Sort final results  \n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]  \n",
    "        final_results = [(self.documents[idx], score) for idx, score in sorted_results]  \n",
    "  \n",
    "        return final_results, time.time() - start_time  \n",
    "  \n",
    "    def compare_search(self, query: str, k: int = 5):  \n",
    "      \"\"\"Show semantic vs keyword results, and final merged hybrid results without double-calling.\"\"\"  \n",
    "    \n",
    "      # ---- Run searches ONCE ----  \n",
    "      semantic_results = self.vector_store.similarity_search_with_relevance_scores(query, k=k*2)  \n",
    "      keyword_results = self.keyword_search(query, k=k*2)  \n",
    "    \n",
    "      # ---- Convert to dictionaries ----  \n",
    "      semantic_scores = {}  \n",
    "      for doc, score in semantic_results:  \n",
    "          idx = self.doc_id_to_index.get(doc.metadata.get('document_id'))  \n",
    "          if idx is not None:  \n",
    "              semantic_scores[idx] = float(score)  \n",
    "    \n",
    "      keyword_scores = {idx: score for idx, score in keyword_results}  \n",
    "    \n",
    "      # ---- Normalize scores ----  \n",
    "      semantic_scores = self._normalize_scores(semantic_scores)  \n",
    "      keyword_scores = self._normalize_scores(keyword_scores)  \n",
    "    \n",
    "      # ---- Build top-k lists for display ----  \n",
    "      semantic_list = [  \n",
    "          {\"rank\": rank, \"doc_id\": doc.metadata.get(\"document_id\", \"N/A\"), \"score\": round(float(score), 3)}  \n",
    "          for rank, (doc, score) in enumerate(semantic_results[:k], start=1)  \n",
    "      ]  \n",
    "      keyword_list = [  \n",
    "          {\"rank\": rank, \"doc_id\": self.documents[idx].metadata.get(\"document_id\", \"N/A\"), \"score\": round(float(score), 3)}  \n",
    "          for rank, (idx, score) in enumerate(keyword_results[:k], start=1)  \n",
    "      ]  \n",
    "    \n",
    "      # ---- Print comparison table ----  \n",
    "      print(f\"\\n🔍 Query: {query}\")  \n",
    "      print(f\"{'Semantic Search':<30} | {'Keyword Search (BM25)':<30}\")  \n",
    "      print(\"-\" * 65)  \n",
    "      for i in range(max(len(semantic_list), len(keyword_list))):  \n",
    "          sem = semantic_list[i] if i < len(semantic_list) else {}  \n",
    "          key = keyword_list[i] if i < len(keyword_list) else {}  \n",
    "          print(f\"{sem.get('rank',''):>2}. DocID {sem.get('doc_id',''):>5} ({sem.get('score','')})\"  \n",
    "                f\" | {key.get('rank',''):>2}. DocID {key.get('doc_id',''):>5} ({key.get('score','')})\")  \n",
    "    \n",
    "      # ---- Merge scores once (Hybrid) ----  \n",
    "      combined_scores = {}  \n",
    "      for idx in set(semantic_scores.keys()) | set(keyword_scores.keys()):  \n",
    "          combined_scores[idx] = (  \n",
    "              self.semantic_weight * semantic_scores.get(idx, 0.0) +  \n",
    "              self.keyword_weight * keyword_scores.get(idx, 0.0)  \n",
    "          )  \n",
    "    \n",
    "      merged_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]  \n",
    "    \n",
    "      print(\"\\n📊 Final Hybrid Results:\")  \n",
    "      for rank, (idx, score) in enumerate(merged_results, start=1):  \n",
    "          print(f\"{rank}. DocID {self.documents[idx].metadata.get('document_id','N/A')} - Score: {round(score,3)}\")  \n",
    "    \n",
    "      return semantic_list, keyword_list, merged_results  \n",
    "      \n",
    "# =============================  \n",
    "# 🚀 Initialize Hybrid Search with BM25  \n",
    "# =============================  \n",
    "print(\"🚀 Initializing Hybrid Search with BM25...\")  \n",
    "  \n",
    "# Create the hybrid search instance  \n",
    "hybrid = HybridSearchStrategy(vector_store, processor)  \n",
    "  \n",
    "query = \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"  \n",
    "hybrid.compare_search(query, k=3)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16392b06",
   "metadata": {},
   "source": [
    "### Strategy 4: LLM-based Re-ranking with Explanations  \n",
    "  \n",
    "**Purpose:**    \n",
    "This strategy uses a **Large Language Model (GPT-4)** to refine search results by re-ranking retrieved chunks according to their direct relevance to the query. While the initial retrieval is done via semantic search, the LLM evaluates the content holistically, prioritizing chunks that most clearly address the user’s question. An explanation is also generated to provide transparency behind the ranking decisions.  \n",
    "  \n",
    "**Evaluation Summary:**    \n",
    "- **Query:** \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"    \n",
    "- **Process:**    \n",
    "  1. Retrieve top 10 results via semantic search.    \n",
    "  2. Present them to GPT-4 with instructions to rank the top 3 by relevance and explain the reasoning.    \n",
    "  3. Extract the ranking order and explanation for output.  \n",
    "- **Final LLM Ranking:**    \n",
    "  1. **Chunk 2 – Wolfram Syndrome:** Directly discusses high blood sugar from insulin deficiency and explicitly mentions insulin replacement therapy.    \n",
    "  2. **Chunk 1 – Donohue Syndrome:** Details severe insulin resistance and its impact on glucose regulation, indirectly relevant to therapies.    \n",
    "  3. **Chunk 4 – Glycogen Storage Disease Type VI:** Primarily about low blood sugar (hypoglycemia); less relevant to the question focus.  \n",
    "  \n",
    "**Observations:**    \n",
    "- LLM re-ranking improved precision by placing therapy-relevant content (Wolfram syndrome with insulin therapy) at the top.    \n",
    "- Provided clear reasoning for each choice, aiding interpretability.    \n",
    "- Reduced the presence of tangentially related results in the top set.  \n",
    "  \n",
    "**Key Takeaway:**    \n",
    "LLM re-ranking is effective for **precision-focused retrieval**, especially when the goal is to directly answer a specific question. The explanation feature enhances trust and transparency, making it valuable in high-stakes domains like healthcare.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt: str, model: str = \"gpt-4\") -> str:  \n",
    "    \"\"\"  \n",
    "    Sends a prompt to the OpenAI API and returns the model's text response.  \n",
    "    \"\"\"  \n",
    "    response = chat_client.chat.completions.create(  \n",
    "        model=model,  \n",
    "        messages=[  \n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},  \n",
    "            {\"role\": \"user\", \"content\": prompt}  \n",
    "        ],  \n",
    "        temperature=0  \n",
    "    )  \n",
    "    return response.choices[0].message.content.strip()  \n",
    "  \n",
    "def llm_rerank_with_explanation(query: str, top_k: int = 3, retrieved_k: int = 10):  \n",
    "    \"\"\"  \n",
    "    Retrieves documents from ChromaDB and reranks them using GPT, with explanations.  \n",
    "      \n",
    "    Args:  \n",
    "        query (str): The user’s input question.  \n",
    "        top_k (int): Number of top chunks to return after reranking.  \n",
    "        retrieved_k (int): Number of chunks to initially retrieve from vector DB.  \n",
    "    \"\"\"  \n",
    "    # Step 1: Retrieve docs from vector DB  \n",
    "    retrieved_docs = vector_store.similarity_search(query, k=retrieved_k)  \n",
    "  \n",
    "    # Step 2: Prepare ranking prompt with explanation request  \n",
    "    prompt = (  \n",
    "        f\"You are helping rank document chunks based on how well they answer this question:\\n\\n\"  \n",
    "        f\"Question: {query}\\n\\n\"  \n",
    "        \"Here are the chunks:\\n\\n\"  \n",
    "    )  \n",
    "  \n",
    "    for i, doc in enumerate(retrieved_docs):  \n",
    "        prompt += f\"Chunk {i+1}:\\n{doc.page_content.strip()}\\n\\n\"  \n",
    "  \n",
    "    prompt += (  \n",
    "        f\"Please rank the top {top_k} chunks in order of relevance.\\n\"  \n",
    "        \"First, give the ranking in this exact format:\\n\"  \n",
    "        \"Ranking: Chunk 3, Chunk 1, Chunk 5\\n\"  \n",
    "        \"Then on the next lines, explain briefly why you chose that order.\"  \n",
    "    )  \n",
    "  \n",
    "    # Step 3: Call GPT for reranking with explanation  \n",
    "    gpt_output = get_response(prompt)  \n",
    "    print(\"GPT Rerank + Explanation Output:\\n\", gpt_output)  \n",
    "  \n",
    "    # Step 4: Extract chunk numbers from GPT output  \n",
    "    ranking_line = next((line for line in gpt_output.split(\"\\n\") if line.startswith(\"Ranking:\")), \"\")  \n",
    "    chunk_order = [  \n",
    "        int(s.strip().split()[-1]) - 1  \n",
    "        for s in ranking_line.replace(\"Ranking:\", \"\").split(',')  \n",
    "        if s.strip().startswith(\"Chunk\")  \n",
    "    ]  \n",
    "  \n",
    "    # Step 5: Extract explanation (everything after the ranking line)  \n",
    "    explanation_lines = []  \n",
    "    found_ranking = False  \n",
    "    for line in gpt_output.split(\"\\n\"):  \n",
    "        if found_ranking:  \n",
    "            explanation_lines.append(line)  \n",
    "        if line.startswith(\"Ranking:\"):  \n",
    "            found_ranking = True  \n",
    "    explanation = \"\\n\".join(explanation_lines).strip()  \n",
    "  \n",
    "    # Step 6: Return sorted chunk objects and explanation  \n",
    "    reranked_docs = [  \n",
    "        retrieved_docs[i]  \n",
    "        for i in chunk_order  \n",
    "        if 0 <= i < len(retrieved_docs)  \n",
    "    ]  \n",
    "  \n",
    "    return reranked_docs, explanation  \n",
    "  \n",
    "query = \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"  \n",
    "top_docs, reasoning = llm_rerank_with_explanation(query, top_k=3)  \n",
    "  \n",
    "print(\"\\nTop Ranked Chunks:\")  \n",
    "for idx, d in enumerate(top_docs, 1):  \n",
    "  print(f\"{idx}. {d.page_content}\")  \n",
    "  \n",
    "print(\"\\nExplanation for ranking:\\n\", reasoning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac649d",
   "metadata": {},
   "source": [
    "### Strategy 5: Hybrid Search + LLM-based Re-ranking with Explanations  \n",
    "  \n",
    "**Purpose:**    \n",
    "This strategy combines the **breadth and balance** of Hybrid Search (semantic + BM25 keyword weighting) with the **precision and interpretability** of LLM-based re-ranking. The hybrid retrieval ensures that results capture both contextual meaning and exact keyword matches, while the LLM (GPT-4) reorders them by evaluating their direct relevance to the query and providing a rationale for the ranking.  \n",
    "  \n",
    "**Evaluation Summary:**    \n",
    "- **Query:** \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"    \n",
    "- **Process:**    \n",
    "  1. Retrieve top 10 results using Hybrid Search (semantic weight 0.7, keyword weight 0.3).    \n",
    "  2. Provide results (with hybrid scores) to GPT-4 for re-ranking and reasoning.    \n",
    "  3. Extract top 3 most relevant chunks based on LLM judgment.    \n",
    "- **Final LLM Ranking:**    \n",
    "  1. **Chunk 2 – Wolfram Syndrome:** Directly addresses high blood sugar from insulin deficiency; explicitly mentions insulin replacement therapy.    \n",
    "  2. **Chunk 1 – Donohue Syndrome:** Describes severe insulin resistance and glucose regulation issues; relevant to early signs but less on therapies.    \n",
    "  3. **Chunk 3 – Glycogen Storage Disease Type VI:** Discusses blood sugar regulation issues but focuses on hypoglycemia, making it less directly relevant.  \n",
    "  \n",
    "**Observations:**    \n",
    "- Hybrid retrieval ensured that all top-ranked documents were at least partially relevant, reducing the risk of missing key keyword-matching results.    \n",
    "- LLM re-ranking improved **precision** by prioritizing therapy-specific and directly relevant content.    \n",
    "- The explanation step increases transparency, which is especially important for clinical or high-stakes contexts.  \n",
    "  \n",
    "**Key Takeaway:**    \n",
    "This combined approach offers **broad recall**, **high precision**, and **explainability**, making it a strong candidate for workflows requiring both comprehensive retrieval and tight alignment with the user’s query intent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_rerank_hybrid_with_explanation(query: str, top_k: int = 3, retrieved_k: int = 10, model: str = \"gpt-4\"):  \n",
    "    \"\"\"  \n",
    "    Runs hybrid search (semantic + BM25), then reranks the results with GPT, returning top_k chunks + explanation.  \n",
    "  \n",
    "    Args:  \n",
    "        query (str): The user’s input question.  \n",
    "        top_k (int): Number of top chunks to return after reranking.  \n",
    "        retrieved_k (int): Number of chunks to initially retrieve from hybrid search.  \n",
    "        model (str): GPT model name (default: gpt-4).  \n",
    "    \"\"\"  \n",
    "  \n",
    "    # Step 1: Retrieve docs using HybridSearchStrategy  \n",
    "    hybrid_results, _ = hybrid.search(query, k=retrieved_k)  # already returns [(Document, score)]  \n",
    "  \n",
    "    # Step 2: Prepare ranking prompt with explanation request  \n",
    "    prompt = (  \n",
    "        f\"You are helping rank document chunks based on how well they answer this question:\\n\\n\"  \n",
    "        f\"Question: {query}\\n\\n\"  \n",
    "        \"Here are the chunks:\\n\\n\"  \n",
    "    )  \n",
    "  \n",
    "    for i, (doc, score) in enumerate(hybrid_results):  \n",
    "        prompt += f\"Chunk {i+1} (Hybrid Score: {round(score,3)}):\\n{doc.page_content.strip()}\\n\\n\"  \n",
    "  \n",
    "    prompt += (  \n",
    "        f\"Please rank the top {top_k} chunks in order of relevance.\\n\"  \n",
    "        \"First, give the ranking in this exact format:\\n\"  \n",
    "        \"Ranking: Chunk 3, Chunk 1, Chunk 5\\n\"  \n",
    "        \"Then on the next lines, explain briefly why you chose that order.\"  \n",
    "    )  \n",
    "  \n",
    "    # Step 3: Call GPT for reranking with explanation  \n",
    "    gpt_output = get_response(prompt, model=model)  \n",
    "    print(\"📊 GPT Rerank + Explanation Output:\\n\", gpt_output)  \n",
    "  \n",
    "    # Step 4: Extract chunk numbers from GPT output  \n",
    "    ranking_line = next((line for line in gpt_output.split(\"\\n\") if line.startswith(\"Ranking:\")), \"\")  \n",
    "    chunk_order = [  \n",
    "        int(s.strip().split()[-1]) - 1  \n",
    "        for s in ranking_line.replace(\"Ranking:\", \"\").split(',')  \n",
    "        if s.strip().startswith(\"Chunk\")  \n",
    "    ]  \n",
    "  \n",
    "    # Step 5: Extract explanation (everything after the ranking line)  \n",
    "    explanation_lines = []  \n",
    "    found_ranking = False  \n",
    "    for line in gpt_output.split(\"\\n\"):  \n",
    "        if found_ranking:  \n",
    "            explanation_lines.append(line)  \n",
    "        if line.startswith(\"Ranking:\"):  \n",
    "            found_ranking = True  \n",
    "    explanation = \"\\n\".join(explanation_lines).strip()  \n",
    "  \n",
    "    # Step 6: Return sorted chunk objects and explanation  \n",
    "    reranked_docs = [  \n",
    "        hybrid_results[i][0]  # only the Document object  \n",
    "        for i in chunk_order  \n",
    "        if 0 <= i < len(hybrid_results)  \n",
    "    ]  \n",
    "  \n",
    "    return reranked_docs, explanation  \n",
    "\n",
    "query = \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"  \n",
    "  \n",
    "top_docs, reasoning = llm_rerank_hybrid_with_explanation(query, top_k=3, retrieved_k=10)  \n",
    "  \n",
    "print(\"\\n🏆 Top Ranked Chunks:\")  \n",
    "for idx, d in enumerate(top_docs, 1):  \n",
    "    print(f\"{idx}. {d.page_content[:200]}...\")  # truncate for display  \n",
    "  \n",
    "print(\"\\n💡 Explanation for ranking:\\n\", reasoning)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa3262",
   "metadata": {},
   "source": [
    "## 📊 Comparative Summary of Retrieval Strategies (Corrected)  \n",
    "  \n",
    "| **Strategy** | **Description** | **Strengths** | **Limitations** | **Top 3 for Query** (\"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\") | **Suitability** |  \n",
    "|--------------|----------------|---------------|------------------|----------------------------------------------------------------------------------------------------------------|------------------|  \n",
    "| **1. Semantic Search** | Retrieves results based on vector similarity of embeddings. | Captures contextual meaning, good for broader queries. | May return tangential matches if semantic similarity is high but exact keyword match is missing. | 1. Donohue Syndrome (DocID 297) <br> 2. Wolfram Syndrome (DocID 328) <br> 3. Glucose-Galactose Malabsorption (DocID 414) | Good for exploratory search when conceptual relevance is key. |  \n",
    "| **2. Keyword Search (BM25)** | Ranks documents based on exact keyword matches and term frequency. | Strong for precision with exact terms; interpretable scoring. | Misses semantically relevant docs without exact term match. | 1. Donohue Syndrome (DocID 297) <br> 2. Wolfram Syndrome (DocID 328) <br> *(At stricter thresholds, only top 1 or 2 are kept)* | Good when terminology is consistent and exact match is critical. |  \n",
    "| **3. Hybrid Search (Semantic + BM25)** | Weighted combination of semantic and keyword scores. | Balances contextual relevance with keyword precision; reduces false positives from pure semantic search. | Weight tuning required; still may include partially relevant docs. |  1. Donohue Syndrome <br> 2. Wolfram Syndrome <br> 3. Glycogen Storage Disease VI | Strong default choice for balanced recall & precision. |  \n",
    "| **4. LLM-based Re-ranking (Semantic Input)** | Retrieves top semantic matches, then reorders with GPT-4 + explanation. | High precision; LLM considers nuance and context; explainable. | Dependent on quality of initial retrieval; higher cost/latency. | 1. Wolfram Syndrome <br> 2. Donohue Syndrome <br> 3. Glycogen Storage Disease VI | Suitable for targeted Q&A where precision and rationale are critical. |  \n",
    "| **5. Hybrid Search + LLM Re-ranking** | Retrieves via Hybrid Search, then reorders with GPT-4 + explanation. | Combines strong recall from hybrid retrieval with high precision from LLM; maximizes relevance; explainable results. | Highest computational cost; requires both retrieval infra + LLM. | 1. Wolfram Syndrome <br> 2. Donohue Syndrome <br> 3. Glycogen Storage Disease VI | **Best overall choice** for high-stakes, precision-critical retrieval. |  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## 🏆 Recommended Best Performing Workflow  \n",
    "  \n",
    "**Recommendation:** **Strategy 5 – Hybrid Search + LLM Re-ranking with Explanations**  \n",
    "  \n",
    "**Why it wins:**  \n",
    "- **Best Recall + Precision Balance:** Hybrid retrieval ensures no highly relevant document is missed while avoiding purely semantic or keyword-only pitfalls.  \n",
    "- **Contextual Judgment:** LLM re-ranking filters and prioritizes chunks that directly answer the question, even when multiple are partially relevant.  \n",
    "- **Transparency:** The explanation step provides clear reasoning, improving trust in the results.  \n",
    "- **Domain Suitability:** Especially effective in clinical/medical contexts where both terminology accuracy and contextual nuance matter.  \n",
    "  \n",
    "**Trade-offs:**    \n",
    "- Higher cost and latency compared to other strategies, but justified for scenarios where **accuracy outweighs performance constraints**.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Final Takeaway:**    \n",
    "> For this assessment, **Hybrid + LLM Re-ranking** offers the most robust, interpretable, and contextually accurate retrieval, making it the **ideal choice for production in high-stakes information retrieval systems**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe35a3",
   "metadata": {},
   "source": [
    "### Final: UnifiedClinicalRAG – End-to-End Retrieval, Re-ranking, and Clinical Answer Generation  \n",
    "  \n",
    "**Purpose:**    \n",
    "The `UnifiedClinicalRAG` pipeline is an **integrated retrieval-augmented generation system** designed for clinical question answering. It combines **hybrid document retrieval** (semantic + keyword matching), **LLM-based re-ranking** for relevance, and **controlled answer generation** that is grounded strictly in retrieved evidence. The pipeline enforces strict rules to avoid hallucination, formats the output as structured JSON, and explicitly includes reasoning, cited evidence IDs, and source URLs.  \n",
    "  \n",
    "**Workflow Highlights:**  \n",
    "- **Step 1 – Retrieval:** Uses hybrid search to gather the most relevant chunks from the document corpus.  \n",
    "- **Step 2 – Re-ranking:** Passes retrieved chunks to an LLM (GPT-4) for ordering by clinical relevance to the query.  \n",
    "- **Step 3 – Answer Generation:** Produces a concise, evidence-grounded clinical answer without exposing internal chunk IDs or numbering in the narrative.  \n",
    "- **Step 4 – Transparency:** Returns reasoning, cited chunk IDs, source links, and stated limitations for interpretability.  \n",
    "- **Step 5 – Latency Tracking:** Captures retrieval, re-ranking, generation, and total processing times for performance monitoring.  \n",
    "  \n",
    "**Evaluation on Sample Query:**    \n",
    "_Query:_ *\"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"_   \n",
    "- **Answer Summary:** Highlighted early signs from Wolfram and Donohue syndromes, with mention of insulin replacement therapy in relevant cases.    \n",
    "- **Sources Returned:**    \n",
    "  - https://ghr.nlm.nih.gov/condition/donohue-syndrome/    \n",
    "  - https://medlineplus.gov/genetics/condition/wolfram-syndrome/    \n",
    "  \n",
    "**Key Takeaway:**    \n",
    "This unified pipeline is well-suited for **precision-critical, high-trust clinical retrieval tasks**, providing structured, explainable outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict  \n",
    "import json, re, statistics, time  \n",
    "from typing import List, Dict, Any, Optional, Tuple  \n",
    "  \n",
    "# --- Retrieved Chunk & Response Data Classes ---  \n",
    "@dataclass  \n",
    "class RetrievedChunk:  \n",
    "    chunk_id: int  \n",
    "    document_id: str  \n",
    "    source: str  \n",
    "    content: str  \n",
    "    score: float  \n",
    "  \n",
    "@dataclass  \n",
    "class GeneratedResponse:  \n",
    "    query: str  \n",
    "    status: str  \n",
    "    answer: str  \n",
    "    reasoning: str  \n",
    "    cited_chunks: List[int]  \n",
    "    sources: List[str]  \n",
    "    limitations: List[str]  \n",
    "    retrieval_latency_sec: float  \n",
    "    rerank_latency_sec: float  \n",
    "    generation_latency_sec: float  \n",
    "    total_latency_sec: float  \n",
    "  \n",
    "# --- Rerank + Answer Prompts ---  \n",
    "RERANK_SYSTEM = (  \n",
    "    \"You are a clinical evidence ranking assistant. \"  \n",
    "    \"Rank ONLY by usefulness to answer the given question. \"  \n",
    "    \"Do not use any external knowledge — base ranking strictly on provided chunks.\"  \n",
    ")  \n",
    "  \n",
    "ANSWER_SYSTEM = (  \n",
    "    \"You are a clinical answering assistant. \"  \n",
    "    \"Generate responses strictly based on the provided evidence excerpts. \"  \n",
    "    \"Do NOT use any external knowledge or LLM internal training data. \"  \n",
    "    \"If the question is unrelated, mark as out_of_scope. \"  \n",
    "    \"If insufficient evidence is provided, mark as insufficient_context. \"  \n",
    "    \"If partially supported, mark as partial. \"  \n",
    "    \"If fully supported, mark as complete. \"  \n",
    "    \"Never hallucinate or speculate — only include facts present in the excerpts.\"  \n",
    ")  \n",
    "  \n",
    "RERANK_TEMPLATE = \"\"\"Question: {query}  \n",
    "Evidence Chunks (structured JSON):  \n",
    "{chunk_block}  \n",
    "Return ranking in format:  \n",
    "Ranking: Chunk i, Chunk j, Chunk k  \n",
    "\"\"\"  \n",
    "  \n",
    "ANSWER_TEMPLATE = \"\"\"Question: {query}  \n",
    "  \n",
    "Evidence Excerpts (each with chunk_id, source, and content):  \n",
    "{chunk_block}  \n",
    "  \n",
    "Create a grounded JSON response following this schema:  \n",
    "{{  \n",
    "  \"status\": \"complete|partial|insufficient_context|out_of_scope\",  \n",
    "  \"answer\": \"string (natural prose, DO NOT mention 'chunk', 'excerpt id', or numeric evidence IDs)\",  \n",
    "  \"reasoning\": \"concise justification without exposing internal chunk numbering\",  \n",
    "  \"cited_chunks\": [list of integer chunk_ids from the evidence above that directly support your answer],  \n",
    "  \"sources\": [list of unique source URLs from the cited chunks],  \n",
    "  \"limitations\": [explicit gaps / uncertainties]  \n",
    "}}  \n",
    "\n",
    "**Special rules**:    \n",
    "- If there are no relevant source excerpts for answering the question, set `\"status\": \"insufficient_context\"` and clearly state in `\"answer\"` that the question cannot be answered using the available sources.    \n",
    "  \n",
    "**Important rules for cited_chunks**:  \n",
    "- ONLY use chunk_id numbers from the Evidence Excerpts above.  \n",
    "- Output them as integers (e.g., [1, 2]) — not strings.  \n",
    "- Include ALL chunk_ids that contributed factual information to the answer.  \n",
    "  \n",
    "**Rules for the answer**:  \n",
    "- DO NOT mention chunk numbers in the answer prose.  \n",
    "- Use only the provided evidence — no external info.  \n",
    "- Clearly state limitations when data is missing.  \n",
    "  \n",
    "Return ONLY the JSON object.  \n",
    "\"\"\"  \n",
    "  \n",
    "JSON_BLOCK_RE = re.compile(r\"```(?:json)?\\s*({[\\s\\S]*?})\\s*```\", re.IGNORECASE)  \n",
    "  \n",
    "# --- Utility: Extract JSON ---  \n",
    "def extract_json(text: str) -> Optional[Dict[str, Any]]:  \n",
    "    match = JSON_BLOCK_RE.search(text)  \n",
    "    candidate = match.group(1) if match else text.strip()  \n",
    "    try:  \n",
    "        return json.loads(candidate)  \n",
    "    except Exception:  \n",
    "        return None  \n",
    "  \n",
    "# --- Response Generator (Unified) ---  \n",
    "class UnifiedClinicalRAG:  \n",
    "    def __init__(self, processor, vector_store, hybrid, chat_client, max_context_chunks: int = 12):  \n",
    "        self.processor = processor  \n",
    "        self.vector_store = vector_store  \n",
    "        self.hybrid = hybrid  \n",
    "        self.chat_client = chat_client  \n",
    "        self.max_context_chunks = max_context_chunks  \n",
    "  \n",
    "    def _llm(self, system: str, user: str, temperature: float = 0) -> str:  \n",
    "        resp = self.chat_client.chat.completions.create(  \n",
    "            model=CHAT_DEPLOYMENT_NAME,  \n",
    "            messages=[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}],  \n",
    "            temperature=temperature  \n",
    "        )  \n",
    "        return resp.choices[0].message.content.strip()  \n",
    "  \n",
    "    def hybrid_retrieve(self, query: str, k: int = 12, score_threshold: float = 0.2) -> Tuple[List[RetrievedChunk], float]:  \n",
    "        t0 = time.time()  \n",
    "        results, _ = self.hybrid.search(query, k=k)  \n",
    "        retrieved = []  \n",
    "        for idx, (doc, score) in enumerate(results):  \n",
    "            if score_threshold is not None and score < score_threshold:  \n",
    "                continue  \n",
    "            retrieved.append(  \n",
    "                RetrievedChunk(  \n",
    "                    chunk_id=idx + 1,  \n",
    "                    document_id=str(doc.metadata.get('document_id', 'N/A')),  \n",
    "                    source=str(doc.metadata.get('document_url', '')),  \n",
    "                    content=doc.page_content.strip(),  \n",
    "                    score=float(score)  \n",
    "                )  \n",
    "            )  \n",
    "        return retrieved, time.time() - t0  \n",
    "  \n",
    "    def rerank(self, query: str, chunks: List[RetrievedChunk], top_k: int = 8) -> Tuple[List[RetrievedChunk], float]:  \n",
    "        if not chunks:  \n",
    "            return [], 0.0  \n",
    "        t0 = time.time()  \n",
    "        chunk_dicts = []  \n",
    "        for c in chunks[: self.max_context_chunks]:  \n",
    "            snippet = c.content[:600].replace(\"\\n\", \" \").strip()  \n",
    "            chunk_dicts.append({  \n",
    "                \"chunk_id\": c.chunk_id,  \n",
    "                \"source\": c.source,  \n",
    "                \"content\": snippet  \n",
    "            })  \n",
    "        chunk_block = json.dumps(chunk_dicts, ensure_ascii=False, indent=2)  \n",
    "        prompt = RERANK_TEMPLATE.format(query=query, chunk_block=chunk_block)  \n",
    "        raw = self._llm(RERANK_SYSTEM, prompt, temperature=0)  \n",
    "        ranking_line = next((ln for ln in raw.split('\\n') if ln.lower().startswith('ranking:')), '')  \n",
    "        order = [int(m.group(1)) for part in ranking_line.replace('Ranking:', '').split(',') if (m := re.search(r'(\\d+)', part))]  \n",
    "        if not order:  \n",
    "            order = [c.chunk_id for c in chunks]  \n",
    "        id_map = {c.chunk_id: c for c in chunks}  \n",
    "        reranked = [id_map[i] for i in order if i in id_map][:top_k]  \n",
    "        return reranked, time.time() - t0  \n",
    "  \n",
    "    def _clean_text(self, text: str) -> str:  \n",
    "        if not text:  \n",
    "            return text  \n",
    "        text = re.sub(r'(?i)chunk\\s*\\d+[:\\-]*', '', text)  \n",
    "        text = re.sub(r'\\s{2,}', ' ', text).strip()  \n",
    "        return text  \n",
    "  \n",
    "    def generate_answer(self, query: str, ranked: List[RetrievedChunk]) -> Tuple[GeneratedResponse, float]:  \n",
    "        t0 = time.time()  \n",
    "        if not ranked:  \n",
    "            empty = GeneratedResponse(  \n",
    "                query=query,  \n",
    "                status='insufficient_context',  \n",
    "                answer='No relevant content was retrieved from the corpus.',  \n",
    "                reasoning='No evidence excerpts matched.',  \n",
    "                cited_chunks=[],  \n",
    "                sources=[],  \n",
    "                limitations=['No evidence available in indexed corpus'],  \n",
    "                retrieval_latency_sec=0, rerank_latency_sec=0, generation_latency_sec=0, total_latency_sec=0  \n",
    "            )  \n",
    "            return empty, 0.0  \n",
    "  \n",
    "        # --- Structured chunk JSON for LLM ---  \n",
    "        chunk_dicts = []  \n",
    "        for c in ranked:  \n",
    "            snippet = c.content[:900].replace(\"\\n\", \" \").strip()  \n",
    "            chunk_dicts.append({  \n",
    "                \"chunk_id\": c.chunk_id,  \n",
    "                \"source\": c.source,  \n",
    "                \"content\": snippet  \n",
    "            })  \n",
    "        chunk_block = json.dumps(chunk_dicts, ensure_ascii=False, indent=2)  \n",
    "  \n",
    "        # --- Build prompt ---  \n",
    "        prompt = ANSWER_TEMPLATE.format(query=query, chunk_block=chunk_block)  \n",
    "        print(\"DEBUG prompt:\", prompt)  \n",
    "        raw = self._llm(ANSWER_SYSTEM, prompt, temperature=0)  \n",
    "        print(\"DEBUG raw:\", raw)  \n",
    "  \n",
    "        # --- Parse JSON output ---  \n",
    "        parsed = extract_json(raw) or {}  \n",
    "        status = parsed.get('status', 'insufficient_context')  \n",
    "  \n",
    "        print(\"DEBUG cited_chunks raw:\", parsed.get('cited_chunks'))  \n",
    "  \n",
    "        # --- Normalize cited_chunks ---  \n",
    "        raw_cited = parsed.get('cited_chunks', [])  \n",
    "        cited = []  \n",
    "        for c in raw_cited:  \n",
    "            try:  \n",
    "                cid = int(c)  \n",
    "                cited.append(cid)  \n",
    "            except (ValueError, TypeError):  \n",
    "                continue  \n",
    "  \n",
    "        valid_ids = {c.chunk_id for c in ranked}  \n",
    "        cited = [i for i in cited if i in valid_ids]  \n",
    "  \n",
    "        # --- Fallback from sources ---  \n",
    "        if not cited and parsed.get('sources'):  \n",
    "            id_map = {c.chunk_id: c for c in ranked}  \n",
    "            cited = [cid for cid, ch in id_map.items() if ch.source in parsed['sources']]  \n",
    "  \n",
    "        # --- Sources ---  \n",
    "        sources = list({s for s in parsed.get('sources', []) if isinstance(s, str)})  \n",
    "        if not sources and cited:  \n",
    "            id_map = {c.chunk_id: c for c in ranked}  \n",
    "            sources = [id_map[i].source for i in cited if i in id_map and id_map[i].source]  \n",
    "  \n",
    "        # --- Clean ---  \n",
    "        clean_answer = self._clean_text(parsed.get('answer', ''))[:3000]  \n",
    "        clean_reasoning = self._clean_text(parsed.get('reasoning', ''))[:1200]  \n",
    "        limitations = parsed.get('limitations', [])  \n",
    "        if not isinstance(limitations, list):  \n",
    "            limitations = [str(limitations)]  \n",
    "  \n",
    "        resp = GeneratedResponse(  \n",
    "            query=query,  \n",
    "            status=status,  \n",
    "            answer=clean_answer,  \n",
    "            reasoning=clean_reasoning,  \n",
    "            cited_chunks=cited,  \n",
    "            sources=sources,  \n",
    "            limitations=limitations,  \n",
    "            retrieval_latency_sec=0.0,  \n",
    "            rerank_latency_sec=0.0,  \n",
    "            generation_latency_sec=time.time() - t0,  \n",
    "            total_latency_sec=0.0  \n",
    "        )  \n",
    "        return resp, resp.generation_latency_sec  \n",
    "  \n",
    "    def answer(self, query: str, retrieve_k: int = 12, rerank_k: int = 6) -> GeneratedResponse:  \n",
    "        t0 = time.time()  \n",
    "        chunks, rlat = self.hybrid_retrieve(query, k=retrieve_k)  \n",
    "        ranked, rrlat = self.rerank(query, chunks, top_k=rerank_k)  \n",
    "        answer_obj, glat = self.generate_answer(query, ranked)  \n",
    "        answer_obj.retrieval_latency_sec = rlat  \n",
    "        answer_obj.rerank_latency_sec = rrlat  \n",
    "        answer_obj.generation_latency_sec = glat  \n",
    "        answer_obj.total_latency_sec = time.time() - t0  \n",
    "        return answer_obj  \n",
    "  \n",
    "# --- Simple Evaluation Harness ---  \n",
    "def evaluate_queries(pipeline: UnifiedClinicalRAG, queries: List[str], verbose: bool = True) -> Dict[str, Any]:  \n",
    "    results = []  \n",
    "    for q in queries:  \n",
    "        resp = pipeline.answer(q)  \n",
    "        results.append(asdict(resp))  \n",
    "        if verbose:  \n",
    "            print(f\"Query: {q}\\n  Status: {resp.status} | EvidenceRefs: {len(resp.cited_chunks)} | Sources: {len(resp.sources)} | Total {resp.total_latency_sec:.2f}s\\n\")  \n",
    "    statuses = [r['status'] for r in results]  \n",
    "    metrics = {  \n",
    "        'counts': {s: statuses.count(s) for s in set(statuses)},  \n",
    "        'avg_total_latency_s': round(statistics.fmean([r['total_latency_sec'] for r in results]), 3),  \n",
    "        'avg_cited_chunks': round(statistics.fmean([len(r['cited_chunks']) for r in results]), 2)  \n",
    "    }  \n",
    "    return {'metrics': metrics, 'results': results}  \n",
    "  \n",
    "print(\"✅ UnifiedClinicalRAG class updated with structured chunk formatting and improved cited_chunks handling.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1158d",
   "metadata": {},
   "source": [
    "# Instantiate the unified pipeline (requires prior cells run)\n",
    "unified_pipeline = UnifiedClinicalRAG(\n",
    "    processor=processor,\n",
    "    vector_store=vector_store,\n",
    "    hybrid=hybrid,\n",
    "    chat_client=chat_client,\n",
    "    max_context_chunks=12\n",
    ")\n",
    "\n",
    "# Demo single query (pretty summary)\n",
    "sample_query = \"What are the early signs and available therapies for high blood sugar caused by lack of insulin?\"\n",
    "resp = unified_pipeline.answer(sample_query)\n",
    "print(\"=== Single Query Result ===\")\n",
    "print(f\"Question: {resp.query}\")\n",
    "print(f\"Status  : {resp.status}\")\n",
    "print(f\"Answer  : {resp.answer[:400]}{'...' if len(resp.answer)>400 else ''}\")\n",
    "print(f\"Evidence Items Referenced (IDs): {resp.cited_chunks}\")\n",
    "print(f\"Sources ({len(resp.sources)}):\")\n",
    "for s in resp.sources:\n",
    "    print(f\"  - {s}\")\n",
    "print(\"Latency (s): retrieval={:.2f} rerank={:.2f} gen={:.2f} total={:.2f}\".format(\n",
    "    resp.retrieval_latency_sec, resp.rerank_latency_sec, resp.generation_latency_sec, resp.total_latency_sec\n",
    "))\n",
    "if resp.limitations:\n",
    "    print(\"Limitations:\")\n",
    "    for l in resp.limitations:\n",
    "        print(f\"  - {l}\")\n",
    "\n",
    "# Batch evaluation example with concise table-style output\n",
    "# query_batch = [\n",
    "#     \"How is diabetic ketoacidosis initially managed?\",\n",
    "#     \"List complications of prolonged untreated high blood sugar.\",\n",
    "#     \"Explain beta cell regeneration in type 1 diabetes (if present).\",\n",
    "#     \"What vaccines are recommended in this corpus?\",\n",
    "# ]\n",
    "# summary = evaluate_queries(unified_pipeline, query_batch, verbose=False)\n",
    "\n",
    "# print(\"\\n=== Batch Evaluation Summary ===\")\n",
    "# print(\"Queries Run:\", len(query_batch))\n",
    "# print(\"Status Counts:\")\n",
    "# for k, v in summary['metrics']['counts'].items():\n",
    "#     print(f\"  {k}: {v}\")\n",
    "# print(f\"Avg Total Latency (s): {summary['metrics']['avg_total_latency_s']}\")\n",
    "# print(f\"Avg Evidence Refs   : {summary['metrics']['avg_cited_chunks']}\")\n",
    "\n",
    "# print(\"\\nPer-Query Snapshot:\")\n",
    "# for r in summary['results']:\n",
    "#     print(f\"- [{r['status']:<18}] {r['query'][:65]}{'...' if len(r['query'])>65 else ''} | refs={len(r['cited_chunks'])} | total={r['total_latency_sec']:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
