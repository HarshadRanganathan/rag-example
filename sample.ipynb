{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c8a330",
   "metadata": {},
   "source": [
    "## **Library and Module Imports**  \n",
    "  \n",
    "The following code block imports all the essential libraries and frameworks required for the Clinical Intelligence System. These libraries support environment configuration, AI model integration, vector storage, data loading, evaluation, and performance metrics.  \n",
    "  \n",
    "### **Key Imports and Their Purpose**  \n",
    "  \n",
    "1. **Core AI and Environment Setup**  \n",
    "   - `import openai` – Provides access to OpenAI's API for natural language processing and model integration.  \n",
    "   - `from dotenv import load_dotenv` – Loads environment variables from a `.env` file to securely store API keys and configuration values.  \n",
    "  \n",
    "2. **Vector Storage and Embeddings**  \n",
    "   - `from langchain.vectorstores import Chroma` – Manages vector databases for semantic search and retrieval.  \n",
    "   - `from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI` – Integrates Azure-hosted OpenAI models for embeddings and chat-based language models.  \n",
    "  \n",
    "3. **Document Handling**  \n",
    "   - `from langchain.schema import Document` – Defines structured document objects for processing.  \n",
    "   - `from langchain_community.document_loaders.csv_loader import CSVLoader` – Loads CSV files into a document structure for further processing.  \n",
    "  \n",
    "4. **Utilities and Data Structures**  \n",
    "   - `from typing import List, Tuple, Dict` – Provides type hints for function parameters and return values.  \n",
    "   - `import numpy as np` – Supports numerical operations, arrays, and mathematical computations.  \n",
    "   - `import time` – Enables time tracking and performance measurement.  \n",
    "  \n",
    "5. **Retry Mechanisms**  \n",
    "   - `from tenacity import retry, stop_after_attempt, wait_random_exponential` – Implements robust retry logic to handle API timeouts and transient failures.  \n",
    "  \n",
    "6. **Evaluation Framework**  \n",
    "   - `from deepeval.models.base_model import DeepEvalBaseLLM` – Base class for evaluating large language models.  \n",
    "   - `from deepeval.test_case import LLMTestCase, LLMTestCaseParams` – Defines structured test cases for model evaluation.  \n",
    "   - `from deepeval import evaluate as deepeval_evaluate` – Runs evaluation processes for AI model outputs.  \n",
    "  \n",
    "7. **Evaluation Metrics**  \n",
    "   - `from deepeval.metrics import (ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric, AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric)`    \n",
    "     - **ContextualPrecisionMetric** – Measures the accuracy of retrieved information within the given context.    \n",
    "     - **ContextualRecallMetric** – Measures how much relevant context is retrieved.    \n",
    "     - **ContextualRelevancyMetric** – Evaluates the contextual fit of retrieved information.    \n",
    "     - **AnswerRelevancyMetric** – Assesses how relevant the generated answer is to the question.    \n",
    "     - **FaithfulnessMetric** – Ensures the answer is factually grounded in the source material.    \n",
    "     - **HallucinationMetric** – Detects fabricated or unsupported information in responses.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Summary:**    \n",
    "This set of imports lays the groundwork for:  \n",
    "- **NLP processing** (OpenAI, Azure OpenAI, LangChain)  \n",
    "- **Data management** (Chroma DB, CSV loaders, Document schema)  \n",
    "- **Robustness** (retry mechanisms)  \n",
    "- **Evaluation** (DeepEval metrics and test cases)    \n",
    "These tools collectively enable the system to process clinical queries, retrieve relevant data, generate responses, and evaluate their quality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np  \n",
    "import time\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval import evaluate as deepeval_evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a5099",
   "metadata": {},
   "source": [
    "### Create Model Client and Set Up Authentication\n",
    "\n",
    "The following code initializes the UAIS environment to establish a secure connection with the Azure OpenAI service. It handles authentication by retrieving the necessary access token and configures the embedding function to generate vector representations for input text. This setup enables downstream tasks such as semantic search, similarity comparison, and other embedding-based applications.\n",
    "\n",
    "| Requirement           | Description                                                        |  \n",
    "|-----------------------|--------------------------------------------------------------------|  \n",
    "| Large Language Models (LLM) | OpenAI LLM API (`gpt-4o-mini_2024-07-18`)                           |  \n",
    "| Embedding Models      | Preferred embedding model is `text-embedding-3-small_1`            |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication:\n",
    "import httpx\n",
    "\n",
    "auth = \"https://api.com/oauth2/token\"\n",
    "client_id = dbutils.secrets.get(scope = \"AIML\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"AIML\", key = \"client_secret\")\n",
    "scope = \"https://api.com/.default\"\n",
    "grant_type = \"client_credentials\"\n",
    "async with httpx.AsyncClient() as client:\n",
    "    body = {\n",
    "        \"grant_type\": grant_type,\n",
    "        \"scope\": scope,\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    resp = await client.post(auth, headers=headers, data=body, timeout=120)\n",
    "    token = resp.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "load_dotenv(\"./Data/vars.env\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"MODEL_ENDPOINT\"]\n",
    "OPENAI_API_VERSION = os.environ[\"API_VERSION\"]\n",
    "CHAT_DEPLOYMENT_NAME = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = os.environ[\"EMBEDDINGS_MODEL_NAME\"]\n",
    "\n",
    "chat_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "embeddings_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={ \n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4e58b",
   "metadata": {},
   "source": [
    "## Azure OpenAI Model & Embeddings Setup  \n",
    "  \n",
    "This section initializes the Azure OpenAI resources required for the RAG pipeline:  \n",
    "  \n",
    "- **`AzureChatOpenAI`** – Configures a chat-based LLM endpoint using Azure OpenAI, enabling conversational interactions and contextual responses.  \n",
    "- **`AzureOpenAIEmbeddings`** – Sets up an embedding model to convert text into high-dimensional vectors for semantic search and retrieval.  \n",
    "- Both components share:  \n",
    "  - The same **API version** and **Azure endpoint**.  \n",
    "  - **Azure AD token authentication** for secure access.  \n",
    "  - Custom **`projectId`** in request headers for project-level tracking.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\"projectId\": PROJECT_ID},\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8746e5c",
   "metadata": {},
   "source": [
    "### Tiktoken Cache Configuration\n",
    " \n",
    "> This code sets up a custom cache directory for Tiktoken by defining `TIKTOKEN_CACHE_DIR` as an environment variable.  \n",
    "> Local caching of tokenization results enhances performance by avoiding repeated computation during recurring embedding or tokenization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062218c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken_cache_dir = os.path.abspath(\"./.setup/tiktoken_cache/\")\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = tiktoken_cache_dir\n",
    "\n",
    "# we have to disable telemetry to use ChromaDB\n",
    "# See here for more information: https://docs.trychroma.com/docs/overview/telemetry\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"]=\"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3946aac",
   "metadata": {},
   "source": [
    "## DocumentProcessor Class  \n",
    "  \n",
    "The `DocumentProcessor` class leverages **LangChain's CSVLoader** to efficiently ingest CSV datasets and convert them into LangChain `Document` objects. This design ensures:  \n",
    "  \n",
    "- **Built-in CSV parsing** with automatic conversion to Document objects.    \n",
    "- **Native LangChain document structure** for smooth integration into RAG pipelines.    \n",
    "- **Standardized metadata extraction** adhering to LangChain conventions.    \n",
    "- **Direct compatibility** with LangChain’s ecosystem of loaders, retrievers, and vector stores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Streamlined Document Processing Engine for RAG pipeline using LangChain CSVLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentProcessor.\n",
    "        \"\"\"\n",
    "        self.langchain_docs = []  # List of LangChain Document objects for RAG\n",
    "        \n",
    "    def load_csv_with_langchain(self, csv_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load CSV data using LangChain CSVLoader.\n",
    "        \n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file to load\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        # Configure CSVLoader with our dataset structure\n",
    "        loader = CSVLoader(\n",
    "            file_path=csv_path,\n",
    "            source_column=\"document_url\",  # Use document_url as source\n",
    "            metadata_columns=[\"document_id\", \"document_url\"],  # Include these in metadata\n",
    "            content_columns=[\"context\"]  # Use context as main content\n",
    "        )\n",
    "        \n",
    "        # Load documents using LangChain\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"Successfully loaded {len(documents)} document chunks from CSV using LangChain CSVLoader.\")\n",
    "        return documents\n",
    "        \n",
    "    def load_dataset(self, dataset_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load documents from CSV using LangChain CSVLoader and return Documents ready for vector storage.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Path to the CSV dataset file\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the dataset file is not found\n",
    "            Exception: If there's an error loading the CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(dataset_path):\n",
    "                raise FileNotFoundError(f\"Dataset file not found: {dataset_path}\")\n",
    "            \n",
    "            print(f\"📁 Loading dataset from: {dataset_path}\")\n",
    "            \n",
    "            # Use LangChain CSVLoader \n",
    "            self.langchain_docs = self.load_csv_with_langchain(dataset_path)\n",
    "            \n",
    "            print(f\"✅ Dataset loaded successfully using LangChain CSVLoader\")\n",
    "            print(f\"📚 Loaded {len(self.langchain_docs)} LangChain Document objects\")\n",
    "            \n",
    "            # Display first document for verification\n",
    "            if self.langchain_docs:\n",
    "                print(f\"\\n🔍 Sample LangChain Document:\")\n",
    "                sample_doc = self.langchain_docs[0]\n",
    "                print(f\"   Content: {sample_doc.page_content[:100]}...\")\n",
    "                print(f\"   Metadata: {sample_doc.metadata}\")\n",
    "            \n",
    "            return self.langchain_docs\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"❌ File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading dataset with CSVLoader: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_langchain_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Get the loaded LangChain Documents ready for vector storage.\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        return self.langchain_docs\n",
    "    \n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get the number of loaded documents.\"\"\"\n",
    "        return len(self.langchain_docs)\n",
    "    \n",
    "    def display_dataset_info(self) -> None:\n",
    "        \"\"\"Display essential information about the loaded dataset for RAG.\"\"\"\n",
    "        if not self.langchain_docs:\n",
    "            print(\"⚠️ No documents loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📊 === Dataset Information for RAG Pipeline ===\")\n",
    "        print(f\"📚 Total documents: {len(self.langchain_docs)}\")\n",
    "        \n",
    "        if self.langchain_docs:\n",
    "            # Show sample LangChain document structure\n",
    "            print(f\"\\n📄 === Sample LangChain Document ===\")\n",
    "            sample_doc = self.langchain_docs[0]\n",
    "            print(f\"page_content: {sample_doc.page_content[:200]}...\")\n",
    "            print(f\"metadata: {sample_doc.metadata}\")\n",
    "            \n",
    "            # Show content length statistics\n",
    "            content_lengths = [len(doc.page_content) for doc in self.langchain_docs]\n",
    "            print(f\"\\n📊 === Content Statistics ===\")\n",
    "            print(f\"Average content length: {sum(content_lengths) / len(content_lengths):.0f} characters\")\n",
    "            print(f\"Shortest document: {min(content_lengths)} characters\")\n",
    "            print(f\"Longest document: {max(content_lengths)} characters\")\n",
    "\n",
    "print(\"✅ LangChain CSVLoader-based DocumentProcessor defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775d180",
   "metadata": {},
   "source": [
    "## Vector Store Creation with Check-and-Reuse Logic  \n",
    "  \n",
    "This section defines a **smart vector store creation pipeline** for efficient embedding management.  \n",
    "  \n",
    "- **Purpose:**    \n",
    "  - Checks if a persisted ChromaDB vector store already exists.    \n",
    "  - **If found:** Loads and reuses existing embeddings without regeneration.    \n",
    "  - **If not found:** Creates a new vector store from the loaded documents, generates embeddings, and persists the store.  \n",
    "  \n",
    "- **Key Features:**    \n",
    "  - Uses `DocumentProcessor` to retrieve preloaded LangChain documents.    \n",
    "  - Embeddings are created via the configured Azure OpenAI embedding model.    \n",
    "  - Ensures persistence in a specified directory for later reuse.    \n",
    "  - Includes diagnostic statistics (vector count, embedding dimensions, range, and norms).    \n",
    "  - Built with retry logic to handle transient failures during creation or loading.  \n",
    "  \n",
    "- **Benefit:**    \n",
    "  This approach saves computation time and cost by **avoiding redundant embedding generation** while still supporting full regeneration when required.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3aa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Creation Pipeline with Check-and-Reuse Logic\n",
    "@retry(wait=wait_random_exponential(min=45, max=120), stop=stop_after_attempt(6))\n",
    "def create_vector_store(processor: DocumentProcessor, collection_name: str = \"clinical_intelligence\", persist_directory: str = \"./Data/clinical_rag.db\"):\n",
    "    \"\"\"\n",
    "    Create embeddings and store in ChromaDB vector store with intelligent reuse.\n",
    "    \n",
    "    Purpose:\n",
    "    - Checks whether a vector store already exists in the specified directory\n",
    "    - If it does, loads and reuses the existing vector store\n",
    "    - If it doesn't, creates a new vector store from the provided documents and persists it\n",
    "    \n",
    "    Args:\n",
    "        processor (DocumentProcessor): Loaded document processor\n",
    "        collection_name (str): Name for the ChromaDB collection\n",
    "        persist_directory (str): Directory path for ChromaDB persistence\n",
    "        \n",
    "    Returns:\n",
    "        Chroma: ChromaDB vector store with embedded documents\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 === Clinical Intelligence Pipeline - Step 2: Vector Store Creation ===\")\n",
    "    print(f\"📂 Checking persist directory: {persist_directory}\")\n",
    "    print(f\"🗂️ Collection name: {collection_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if vector store already exists\n",
    "        if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "            print(f\"📋 Existing vector store found in: {persist_directory}\")\n",
    "            print(f\"🔄 Loading existing vector store...\")\n",
    "            \n",
    "            # Load existing vector store\n",
    "            vector_store = Chroma(\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Existing vector store loaded successfully!\")\n",
    "            print(f\"📚 Found {vector_store._collection.count()} existing vectors\")\n",
    "            print(f\"🔄 Reusing existing embeddings (no regeneration needed)\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"📭 No existing vector store found\")\n",
    "            print(f\"🆕 Creating new vector store from documents...\")\n",
    "            print(f\"📊 Processing {processor.get_document_count()} documents for embedding...\")\n",
    "            \n",
    "            # Get documents from processor\n",
    "            documents = processor.get_langchain_documents()\n",
    "            \n",
    "            if not documents:\n",
    "                raise ValueError(\"No documents loaded. Run clinical_intelligence_pipeline first.\")\n",
    "            \n",
    "            # Create new vector store with embeddings\n",
    "            print(f\"🔍 Creating embeddings using text-embedding-3-small model...\")\n",
    "            print(f\"💾 Storing vectors in ChromaDB collection: '{collection_name}'\")\n",
    "            \n",
    "            # Create vector store - this will automatically generate embeddings and store them\n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=embeddings,\n",
    "                collection_name=collection_name,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ New vector store created successfully!\")\n",
    "            print(f\"📚 Embedded and stored {len(documents)} documents\")\n",
    "            print(f\"💾 Persisted to: {persist_directory}\")\n",
    "        \n",
    "        # Display common statistics\n",
    "        print(f\"\\n📊 === Vector Store Statistics ===\")\n",
    "        print(f\"Total vectors: {vector_store._collection.count()}\")\n",
    "        print(f\"Collection name: {collection_name}\")\n",
    "        print(f\"Persist directory: {persist_directory}\")\n",
    "        print(f\"Embedding model: text-embedding-3-small\")\n",
    "        \n",
    "        # Test embedding dimension using actual document content from CSV\n",
    "        try:\n",
    "            # Use first document's content for testing embedding\n",
    "            documents = processor.get_langchain_documents()\n",
    "            if documents and len(documents) > 0:\n",
    "                # Take first 100 characters from first document for testing\n",
    "                test_content = documents[0].page_content[:100]\n",
    "                test_embedding = embeddings.embed_query(test_content)\n",
    "                print(f\"Vector dimension: {len(test_embedding)}\")\n",
    "                print(f\"Test content: '{test_content[:50]}...'\")\n",
    "                print(f\"Sample embedding values (first 10): {test_embedding[:10]}\")\n",
    "                print(f\"Embedding range: [{min(test_embedding):.6f}, {max(test_embedding):.6f}]\")\n",
    "                print(f\"Embedding norm: {sum(x*x for x in test_embedding)**0.5:.6f}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No documents available for embedding test\")\n",
    "                print(f\"Vector dimension: Unknown (no test performed)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Vector dimension: Available in collection (test failed: {e})\")\n",
    "        \n",
    "        return vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with vector store: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"✅ Smart vector store creation function defined successfully!\")\n",
    "print(\"🔄 Supports both new creation and existing vector store reuse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccca888",
   "metadata": {},
   "source": [
    "## Complete Clinical Intelligence Pipeline  \n",
    "  \n",
    "This function orchestrates the **full data-to-vector pipeline** for the Clinical Intelligence system, combining both document ingestion and vectorization.  \n",
    "  \n",
    "- **Purpose:**    \n",
    "  - Streamline the process of loading CSV-based clinical data into LangChain `Document` objects.    \n",
    "  - Automatically embed documents into a ChromaDB vector store for semantic search and retrieval.  \n",
    "  \n",
    "- **Workflow Steps:**    \n",
    "  1. **Document Processing:**    \n",
    "     - Instantiates a `DocumentProcessor`.    \n",
    "     - Loads the dataset via LangChain's `CSVLoader`.    \n",
    "     - Prepares documents for downstream vectorization.  \n",
    "  2. **Vector Store Creation:**    \n",
    "     - Invokes the smart `create_vector_store()` function.    \n",
    "     - Either reuses an existing ChromaDB store or generates embeddings for new documents.    \n",
    "     - Persists the vector store for future queries.  \n",
    "  \n",
    "- **Output:**    \n",
    "  Returns a tuple of `(DocumentProcessor, Chroma)` for immediate use in query workflows.  \n",
    "  \n",
    "- **Benefit:**    \n",
    "  Offers a **one-command execution** for setting up the RAG-ready clinical intelligence environment, ensuring consistency between document loading and embedding stages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Clinical Intelligence Pipeline - Data Loading + Vectorization\n",
    "def complete_clinical_intelligence_pipeline(dataset_path: str, persist_directory: str, collection_name: str = \"clinical_intelligence\"):\n",
    "    \"\"\"\n",
    "    Complete Clinical Intelligence Pipeline that handles both document processing and vector store creation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the CSV dataset file\n",
    "        persist_directory (str): Directory path for ChromaDB persistence\n",
    "        collection_name (str): Name for the ChromaDB collection\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DocumentProcessor, Chroma) - processor and vector store\n",
    "    \"\"\"\n",
    "    print(f\"🏥 === Complete Clinical Intelligence Pipeline ===\")\n",
    "    print(f\"📂 Dataset path: {dataset_path}\")\n",
    "    print(f\"💾 Persist directory: {persist_directory}\")\n",
    "    print(f\"🗂️ Collection name: {collection_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Document Processing\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🚀 === Step 1: Document Processing ===\")\n",
    "        print(f\"📂 Dataset path: {dataset_path}\")\n",
    "        \n",
    "        # Initialize the document processor\n",
    "        processor = DocumentProcessor()\n",
    "        \n",
    "        # Load the dataset using LangChain CSVLoader\n",
    "        langchain_documents = processor.load_dataset(dataset_path)\n",
    "        print(f\"\\n🎉 Successfully loaded {processor.get_document_count()} LangChain Documents using CSVLoader!\")\n",
    "        print(f\"✅ Documents are ready for vector storage and embeddings\")\n",
    "        print(f\"🔄 Using same loading pattern as PDF documents for consistency\")\n",
    "        \n",
    "        # Step 2: Vector Store Creation\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🚀 === Step 2: Vector Store Creation ===\")\n",
    "        vector_store = create_vector_store(\n",
    "            processor, \n",
    "            collection_name=collection_name, \n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 === Complete Clinical Intelligence Pipeline Finished ===\")\n",
    "        print(f\"✅ Step 1: Document processing completed - {processor.get_document_count()} documents loaded\")\n",
    "        print(f\"✅ Step 2: Vector store creation completed - {vector_store._collection.count()} vectors stored\")\n",
    "        print(f\"🚀 Clinical Intelligence system ready for queries!\")\n",
    "        \n",
    "        return processor, vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in complete pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration variables\n",
    "dataset_path = \"./Data/capstone1_rag_dataset.csv\"\n",
    "persist_directory = \"./Data/clinical_rag.db\"\n",
    "collection_name = \"clinical_intelligence_v1\"\n",
    "\n",
    "# Execute Complete Pipeline in One Call\n",
    "processor, vector_store = complete_clinical_intelligence_pipeline(\n",
    "    dataset_path=dataset_path,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
