{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c8a330",
   "metadata": {},
   "source": [
    "## **Library and Module Imports**  \n",
    "  \n",
    "The following code block imports all the essential libraries and frameworks required for the Clinical Intelligence System. These libraries support environment configuration, AI model integration, vector storage, data loading, evaluation, and performance metrics.  \n",
    "  \n",
    "### **Key Imports and Their Purpose**  \n",
    "  \n",
    "1. **Core AI and Environment Setup**  \n",
    "   - `import openai` – Provides access to OpenAI's API for natural language processing and model integration.  \n",
    "   - `from dotenv import load_dotenv` – Loads environment variables from a `.env` file to securely store API keys and configuration values.  \n",
    "  \n",
    "2. **Vector Storage and Embeddings**  \n",
    "   - `from langchain.vectorstores import Chroma` – Manages vector databases for semantic search and retrieval.  \n",
    "   - `from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI` – Integrates Azure-hosted OpenAI models for embeddings and chat-based language models.  \n",
    "  \n",
    "3. **Document Handling**  \n",
    "   - `from langchain.schema import Document` – Defines structured document objects for processing.  \n",
    "   - `from langchain_community.document_loaders.csv_loader import CSVLoader` – Loads CSV files into a document structure for further processing.  \n",
    "  \n",
    "4. **Utilities and Data Structures**  \n",
    "   - `from typing import List, Tuple, Dict` – Provides type hints for function parameters and return values.  \n",
    "   - `import numpy as np` – Supports numerical operations, arrays, and mathematical computations.  \n",
    "   - `import time` – Enables time tracking and performance measurement.  \n",
    "  \n",
    "5. **Retry Mechanisms**  \n",
    "   - `from tenacity import retry, stop_after_attempt, wait_random_exponential` – Implements robust retry logic to handle API timeouts and transient failures.  \n",
    "  \n",
    "6. **Evaluation Framework**  \n",
    "   - `from deepeval.models.base_model import DeepEvalBaseLLM` – Base class for evaluating large language models.  \n",
    "   - `from deepeval.test_case import LLMTestCase, LLMTestCaseParams` – Defines structured test cases for model evaluation.  \n",
    "   - `from deepeval import evaluate as deepeval_evaluate` – Runs evaluation processes for AI model outputs.  \n",
    "  \n",
    "7. **Evaluation Metrics**  \n",
    "   - `from deepeval.metrics import (ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric, AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric)`    \n",
    "     - **ContextualPrecisionMetric** – Measures the accuracy of retrieved information within the given context.    \n",
    "     - **ContextualRecallMetric** – Measures how much relevant context is retrieved.    \n",
    "     - **ContextualRelevancyMetric** – Evaluates the contextual fit of retrieved information.    \n",
    "     - **AnswerRelevancyMetric** – Assesses how relevant the generated answer is to the question.    \n",
    "     - **FaithfulnessMetric** – Ensures the answer is factually grounded in the source material.    \n",
    "     - **HallucinationMetric** – Detects fabricated or unsupported information in responses.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Summary:**    \n",
    "This set of imports lays the groundwork for:  \n",
    "- **NLP processing** (OpenAI, Azure OpenAI, LangChain)  \n",
    "- **Data management** (Chroma DB, CSV loaders, Document schema)  \n",
    "- **Robustness** (retry mechanisms)  \n",
    "- **Evaluation** (DeepEval metrics and test cases)    \n",
    "These tools collectively enable the system to process clinical queries, retrieve relevant data, generate responses, and evaluate their quality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np  \n",
    "import time\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval import evaluate as deepeval_evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a5099",
   "metadata": {},
   "source": [
    "### Create Model Client and Set Up Authentication\n",
    "\n",
    "The following code initializes the UAIS environment to establish a secure connection with the Azure OpenAI service. It handles authentication by retrieving the necessary access token and configures the embedding function to generate vector representations for input text. This setup enables downstream tasks such as semantic search, similarity comparison, and other embedding-based applications.\n",
    "\n",
    "| Requirement           | Description                                                        |  \n",
    "|-----------------------|--------------------------------------------------------------------|  \n",
    "| Large Language Models (LLM) | OpenAI LLM API (`gpt-4o-mini_2024-07-18`)                           |  \n",
    "| Embedding Models      | Preferred embedding model is `text-embedding-3-small_1`            |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication:\n",
    "import httpx\n",
    "\n",
    "auth = \"https://api.com/oauth2/token\"\n",
    "client_id = dbutils.secrets.get(scope = \"AIML\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"AIML\", key = \"client_secret\")\n",
    "scope = \"https://api.com/.default\"\n",
    "grant_type = \"client_credentials\"\n",
    "async with httpx.AsyncClient() as client:\n",
    "    body = {\n",
    "        \"grant_type\": grant_type,\n",
    "        \"scope\": scope,\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    resp = await client.post(auth, headers=headers, data=body, timeout=120)\n",
    "    token = resp.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "load_dotenv(\"./Data/vars.env\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"MODEL_ENDPOINT\"]\n",
    "OPENAI_API_VERSION = os.environ[\"API_VERSION\"]\n",
    "CHAT_DEPLOYMENT_NAME = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = os.environ[\"EMBEDDINGS_MODEL_NAME\"]\n",
    "\n",
    "chat_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "embeddings_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={ \n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4e58b",
   "metadata": {},
   "source": [
    "## Azure OpenAI Model & Embeddings Setup  \n",
    "  \n",
    "This section initializes the Azure OpenAI resources required for the RAG pipeline:  \n",
    "  \n",
    "- **`AzureChatOpenAI`** – Configures a chat-based LLM endpoint using Azure OpenAI, enabling conversational interactions and contextual responses.  \n",
    "- **`AzureOpenAIEmbeddings`** – Sets up an embedding model to convert text into high-dimensional vectors for semantic search and retrieval.  \n",
    "- Both components share:  \n",
    "  - The same **API version** and **Azure endpoint**.  \n",
    "  - **Azure AD token authentication** for secure access.  \n",
    "  - Custom **`projectId`** in request headers for project-level tracking.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\"projectId\": PROJECT_ID},\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8746e5c",
   "metadata": {},
   "source": [
    "### Tiktoken Cache Configuration\n",
    " \n",
    "> This code sets up a custom cache directory for Tiktoken by defining `TIKTOKEN_CACHE_DIR` as an environment variable.  \n",
    "> Local caching of tokenization results enhances performance by avoiding repeated computation during recurring embedding or tokenization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062218c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken_cache_dir = os.path.abspath(\"./.setup/tiktoken_cache/\")\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = tiktoken_cache_dir\n",
    "\n",
    "# we have to disable telemetry to use ChromaDB\n",
    "# See here for more information: https://docs.trychroma.com/docs/overview/telemetry\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"]=\"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3946aac",
   "metadata": {},
   "source": [
    "## DocumentProcessor Class  \n",
    "  \n",
    "The `DocumentProcessor` class leverages **LangChain's CSVLoader** to efficiently ingest CSV datasets and convert them into LangChain `Document` objects. This design ensures:  \n",
    "  \n",
    "- **Built-in CSV parsing** with automatic conversion to Document objects.    \n",
    "- **Native LangChain document structure** for smooth integration into RAG pipelines.    \n",
    "- **Standardized metadata extraction** adhering to LangChain conventions.    \n",
    "- **Direct compatibility** with LangChain’s ecosystem of loaders, retrievers, and vector stores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Streamlined Document Processing Engine for RAG pipeline using LangChain CSVLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentProcessor.\n",
    "        \"\"\"\n",
    "        self.langchain_docs = []  # List of LangChain Document objects for RAG\n",
    "        \n",
    "    def load_csv_with_langchain(self, csv_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load CSV data using LangChain CSVLoader.\n",
    "        \n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file to load\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        # Configure CSVLoader with our dataset structure\n",
    "        loader = CSVLoader(\n",
    "            file_path=csv_path,\n",
    "            source_column=\"document_url\",  # Use document_url as source\n",
    "            metadata_columns=[\"document_id\", \"document_url\"],  # Include these in metadata\n",
    "            content_columns=[\"context\"]  # Use context as main content\n",
    "        )\n",
    "        \n",
    "        # Load documents using LangChain\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"Successfully loaded {len(documents)} document chunks from CSV using LangChain CSVLoader.\")\n",
    "        return documents\n",
    "        \n",
    "    def load_dataset(self, dataset_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load documents from CSV using LangChain CSVLoader and return Documents ready for vector storage.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Path to the CSV dataset file\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the dataset file is not found\n",
    "            Exception: If there's an error loading the CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(dataset_path):\n",
    "                raise FileNotFoundError(f\"Dataset file not found: {dataset_path}\")\n",
    "            \n",
    "            print(f\"📁 Loading dataset from: {dataset_path}\")\n",
    "            \n",
    "            # Use LangChain CSVLoader \n",
    "            self.langchain_docs = self.load_csv_with_langchain(dataset_path)\n",
    "            \n",
    "            print(f\"✅ Dataset loaded successfully using LangChain CSVLoader\")\n",
    "            print(f\"📚 Loaded {len(self.langchain_docs)} LangChain Document objects\")\n",
    "            \n",
    "            # Display first document for verification\n",
    "            if self.langchain_docs:\n",
    "                print(f\"\\n🔍 Sample LangChain Document:\")\n",
    "                sample_doc = self.langchain_docs[0]\n",
    "                print(f\"   Content: {sample_doc.page_content[:100]}...\")\n",
    "                print(f\"   Metadata: {sample_doc.metadata}\")\n",
    "            \n",
    "            return self.langchain_docs\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"❌ File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading dataset with CSVLoader: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_langchain_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Get the loaded LangChain Documents ready for vector storage.\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        return self.langchain_docs\n",
    "    \n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get the number of loaded documents.\"\"\"\n",
    "        return len(self.langchain_docs)\n",
    "    \n",
    "    def display_dataset_info(self) -> None:\n",
    "        \"\"\"Display essential information about the loaded dataset for RAG.\"\"\"\n",
    "        if not self.langchain_docs:\n",
    "            print(\"⚠️ No documents loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📊 === Dataset Information for RAG Pipeline ===\")\n",
    "        print(f\"📚 Total documents: {len(self.langchain_docs)}\")\n",
    "        \n",
    "        if self.langchain_docs:\n",
    "            # Show sample LangChain document structure\n",
    "            print(f\"\\n📄 === Sample LangChain Document ===\")\n",
    "            sample_doc = self.langchain_docs[0]\n",
    "            print(f\"page_content: {sample_doc.page_content[:200]}...\")\n",
    "            print(f\"metadata: {sample_doc.metadata}\")\n",
    "            \n",
    "            # Show content length statistics\n",
    "            content_lengths = [len(doc.page_content) for doc in self.langchain_docs]\n",
    "            print(f\"\\n📊 === Content Statistics ===\")\n",
    "            print(f\"Average content length: {sum(content_lengths) / len(content_lengths):.0f} characters\")\n",
    "            print(f\"Shortest document: {min(content_lengths)} characters\")\n",
    "            print(f\"Longest document: {max(content_lengths)} characters\")\n",
    "\n",
    "print(\"✅ LangChain CSVLoader-based DocumentProcessor defined successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
